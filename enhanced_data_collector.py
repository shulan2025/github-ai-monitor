#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæ•°æ®æ”¶é›†å™¨ v2.0
å®ç°å®Œæ•´çš„GitHubæŒ‡æ ‡æ”¶é›†å’Œæ™ºèƒ½è¯„åˆ†ç®—æ³•
"""

import os
import json
import time
import requests
from datetime import datetime, timedelta
from cloudflare import Cloudflare
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# APIé…ç½®
GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')
CLOUDFLARE_API_TOKEN = os.environ.get('CLOUDFLARE_API_TOKEN')
CLOUDFLARE_ACCOUNT_ID = os.environ.get('CLOUDFLARE_ACCOUNT_ID')
D1_DATABASE_ID = os.environ.get('D1_DATABASE_ID')

# GitHub APIé…ç½®
GITHUB_HEADERS = {
    'Authorization': f'token {GITHUB_TOKEN}',
    'Accept': 'application/vnd.github.v3+json',
    'User-Agent': 'Enhanced-AI-Repo-Monitor/2.0'
}

# Cloudflareå®¢æˆ·ç«¯
cloudflare_client = Cloudflare(api_token=CLOUDFLARE_API_TOKEN)

def fetch_comprehensive_repo_data(owner, repo_name):
    """è·å–ä»“åº“çš„å®Œæ•´æ•°æ®"""
    
    try:
        print(f"ğŸ“Š æ­£åœ¨è·å– {owner}/{repo_name} çš„å®Œæ•´æ•°æ®...")
        
        # 1. åŸºç¡€ä»“åº“ä¿¡æ¯
        repo_url = f"https://api.github.com/repos/{owner}/{repo_name}"
        repo_response = requests.get(repo_url, headers=GITHUB_HEADERS)
        
        if repo_response.status_code != 200:
            print(f"âŒ è·å–ä»“åº“ä¿¡æ¯å¤±è´¥: {repo_response.status_code}")
            return None
            
        repo_data = repo_response.json()
        
        # 2. è´¡çŒ®è€…ä¿¡æ¯
        contributors = fetch_contributors_count(owner, repo_name)
        
        # 3. æäº¤ä¿¡æ¯  
        commits_info = fetch_commits_info(owner, repo_name)
        
        # 4. Pull Requestsä¿¡æ¯
        prs_info = fetch_prs_info(owner, repo_name)
        
        # 5. Issuesä¿¡æ¯
        issues_info = fetch_issues_info(owner, repo_name)
        
        # 6. å‘å¸ƒä¿¡æ¯
        releases_info = fetch_releases_info(owner, repo_name)
        
        # 7. è¯­è¨€åˆ†å¸ƒ
        languages_info = fetch_languages_info(owner, repo_name)
        
        # 8. å†…å®¹åˆ†æ (README, æ–‡æ¡£ç­‰)
        content_analysis = analyze_repo_content(owner, repo_name)
        
        # 9. AI/MLç‰¹å®šåˆ†æ
        ai_analysis = analyze_ai_features(repo_data, content_analysis)
        
        # æ•´åˆæ‰€æœ‰æ•°æ®
        comprehensive_data = {
            'basic_info': repo_data,
            'contributors': contributors,
            'commits': commits_info,
            'pull_requests': prs_info,
            'issues': issues_info,
            'releases': releases_info,
            'languages': languages_info,
            'content': content_analysis,
            'ai_features': ai_analysis
        }
        
        return comprehensive_data
        
    except Exception as e:
        print(f"âŒ è·å–æ•°æ®æ—¶å‡ºé”™: {e}")
        return None

def fetch_contributors_count(owner, repo_name):
    """è·å–è´¡çŒ®è€…æ•°é‡"""
    try:
        url = f"https://api.github.com/repos/{owner}/{repo_name}/contributors"
        response = requests.get(url, headers=GITHUB_HEADERS)
        
        if response.status_code == 200:
            contributors = response.json()
            return len(contributors)
    except Exception as e:
        print(f"âš ï¸ è·å–è´¡çŒ®è€…å¤±è´¥: {e}")
    
    return 0

def fetch_commits_info(owner, repo_name):
    """è·å–æäº¤ä¿¡æ¯"""
    try:
        # è·å–æ€»æäº¤æ•° (é€šè¿‡æœ€åä¸€é¡µ)
        url = f"https://api.github.com/repos/{owner}/{repo_name}/commits"
        params = {'per_page': 1, 'page': 1}
        response = requests.get(url, headers=GITHUB_HEADERS, params=params)
        
        commits_count = 0
        last_commit_date = None
        
        if response.status_code == 200:
            # ä»Link headerè·å–æ€»é¡µæ•°
            link_header = response.headers.get('Link', '')
            if 'last' in link_header:
                import re
                match = re.search(r'page=(\d+)>; rel="last"', link_header)
                if match:
                    commits_count = int(match.group(1))
            
            # è·å–æœ€æ–°æäº¤ä¿¡æ¯
            commits = response.json()
            if commits:
                last_commit_date = commits[0]['commit']['author']['date']
        
        return {
            'total_commits': commits_count,
            'last_commit_date': last_commit_date
        }
        
    except Exception as e:
        print(f"âš ï¸ è·å–æäº¤ä¿¡æ¯å¤±è´¥: {e}")
        return {'total_commits': 0, 'last_commit_date': None}

def fetch_prs_info(owner, repo_name):
    """è·å–Pull Requestsä¿¡æ¯"""
    try:
        # è·å–PRç»Ÿè®¡
        url = f"https://api.github.com/repos/{owner}/{repo_name}/pulls"
        params = {'state': 'all', 'per_page': 100}
        response = requests.get(url, headers=GITHUB_HEADERS, params=params)
        
        if response.status_code == 200:
            prs = response.json()
            
            open_prs = [pr for pr in prs if pr['state'] == 'open']
            closed_prs = [pr for pr in prs if pr['state'] == 'closed']
            
            return {
                'total_prs': len(prs),
                'open_prs': len(open_prs),
                'closed_prs': len(closed_prs)
            }
    except Exception as e:
        print(f"âš ï¸ è·å–PRä¿¡æ¯å¤±è´¥: {e}")
    
    return {'total_prs': 0, 'open_prs': 0, 'closed_prs': 0}

def fetch_issues_info(owner, repo_name):
    """è·å–Issuesä¿¡æ¯"""
    try:
        # è·å–Issuesç»Ÿè®¡
        url = f"https://api.github.com/repos/{owner}/{repo_name}/issues"
        params = {'state': 'all', 'per_page': 100}
        response = requests.get(url, headers=GITHUB_HEADERS, params=params)
        
        if response.status_code == 200:
            issues = response.json()
            
            # è¿‡æ»¤æ‰Pull Requests (GitHub APIä¸­IssuesåŒ…å«PRs)
            real_issues = [issue for issue in issues if 'pull_request' not in issue]
            open_issues = [issue for issue in real_issues if issue['state'] == 'open']
            
            return {
                'total_issues': len(real_issues),
                'open_issues': len(open_issues)
            }
    except Exception as e:
        print(f"âš ï¸ è·å–Issuesä¿¡æ¯å¤±è´¥: {e}")
    
    return {'total_issues': 0, 'open_issues': 0}

def fetch_releases_info(owner, repo_name):
    """è·å–å‘å¸ƒä¿¡æ¯"""
    try:
        url = f"https://api.github.com/repos/{owner}/{repo_name}/releases"
        response = requests.get(url, headers=GITHUB_HEADERS)
        
        if response.status_code == 200:
            releases = response.json()
            return len(releases)
    except Exception as e:
        print(f"âš ï¸ è·å–å‘å¸ƒä¿¡æ¯å¤±è´¥: {e}")
    
    return 0

def fetch_languages_info(owner, repo_name):
    """è·å–ç¼–ç¨‹è¯­è¨€ä¿¡æ¯"""
    try:
        url = f"https://api.github.com/repos/{owner}/{repo_name}/languages"
        response = requests.get(url, headers=GITHUB_HEADERS)
        
        if response.status_code == 200:
            languages = response.json()
            
            if languages:
                # æ‰¾å‡ºä¸»è¦è¯­è¨€ (ä½¿ç”¨å­—èŠ‚æ•°æœ€å¤šçš„)
                primary_language = max(languages.items(), key=lambda x: x[1])[0]
                
                # æ„å»ºæŠ€æœ¯æ ˆå­—ç¬¦ä¸²
                tech_stack = ', '.join(languages.keys())
                
                return {
                    'primary_language': primary_language,
                    'tech_stack': tech_stack,
                    'languages_count': len(languages)
                }
    except Exception as e:
        print(f"âš ï¸ è·å–è¯­è¨€ä¿¡æ¯å¤±è´¥: {e}")
    
    return {
        'primary_language': 'Unknown',
        'tech_stack': '',
        'languages_count': 0
    }

def analyze_repo_content(owner, repo_name):
    """åˆ†æä»“åº“å†…å®¹ç‰¹å¾"""
    try:
        analysis = {
            'has_readme': False,
            'has_wiki': False,
            'has_pages': False,
            'documentation_score': 0
        }
        
        # æ£€æŸ¥README
        readme_url = f"https://api.github.com/repos/{owner}/{repo_name}/readme"
        readme_response = requests.get(readme_url, headers=GITHUB_HEADERS)
        
        if readme_response.status_code == 200:
            analysis['has_readme'] = True
            readme_data = readme_response.json()
            # READMEè´¨é‡è¯„åˆ† (åŸºäºå¤§å°)
            size = readme_data.get('size', 0)
            analysis['documentation_score'] = min(15, size // 500)  # æ¯500å­—èŠ‚1åˆ†ï¼Œæœ€é«˜15åˆ†
        
        # æ£€æŸ¥ä»“åº“è®¾ç½® (é€šè¿‡åŸºç¡€APIåˆ¤æ–­)
        repo_url = f"https://api.github.com/repos/{owner}/{repo_name}"
        repo_response = requests.get(repo_url, headers=GITHUB_HEADERS)
        
        if repo_response.status_code == 200:
            repo_data = repo_response.json()
            analysis['has_wiki'] = repo_data.get('has_wiki', False)
            analysis['has_pages'] = repo_data.get('has_pages', False)
        
        return analysis
        
    except Exception as e:
        print(f"âš ï¸ å†…å®¹åˆ†æå¤±è´¥: {e}")
        return {
            'has_readme': False,
            'has_wiki': False, 
            'has_pages': False,
            'documentation_score': 0
        }

def analyze_ai_features(repo_data, content_analysis):
    """åˆ†æAI/MLç‰¹å®šç‰¹å¾"""
    
    name = repo_data.get('name', '').lower()
    description = repo_data.get('description', '').lower() if repo_data.get('description') else ''
    topics = repo_data.get('topics', [])
    full_text = f"{name} {description} {' '.join(topics)}"
    
    ai_analysis = {
        'ai_framework': 'unknown',
        'model_type': 'unknown',
        'has_model_files': False,
        'has_paper': False,
        'cutting_edge_score': 0,
        'practical_score': 0
    }
    
    # AIæ¡†æ¶è¯†åˆ«
    frameworks = {
        'pytorch': ['pytorch', 'torch'],
        'tensorflow': ['tensorflow', 'tf'],
        'huggingface': ['huggingface', 'transformers'],
        'langchain': ['langchain'],
        'openai': ['openai', 'gpt'],
        'anthropic': ['claude', 'anthropic'],
        'scikit-learn': ['sklearn', 'scikit-learn'],
        'keras': ['keras'],
        'jax': ['jax', 'flax']
    }
    
    for framework, keywords in frameworks.items():
        if any(keyword in full_text for keyword in keywords):
            ai_analysis['ai_framework'] = framework
            break
    
    # æ¨¡å‹ç±»å‹è¯†åˆ«
    model_types = {
        'llm': ['llm', 'language model', 'gpt', 'bert', 'transformer', 'chatbot'],
        'cv': ['computer vision', 'image', 'detection', 'yolo', 'opencv', 'vision'],
        'nlp': ['nlp', 'natural language', 'text processing', 'sentiment'],
        'ml': ['machine learning', 'classification', 'regression', 'clustering'],
        'dl': ['deep learning', 'neural network', 'cnn', 'rnn', 'lstm'],
        'rag': ['rag', 'retrieval', 'vector database', 'embedding'],
        'agent': ['agent', 'autonomous', 'planning', 'reasoning'],
        'multimodal': ['multimodal', 'vision-language', 'clip'],
        'generative': ['generation', 'gan', 'vae', 'diffusion']
    }
    
    for model_type, keywords in model_types.items():
        if any(keyword in full_text for keyword in keywords):
            ai_analysis['model_type'] = model_type
            break
    
    # æ£€æµ‹æ¨¡å‹æ–‡ä»¶
    model_keywords = ['model', 'checkpoint', 'weights', '.pth', '.h5', '.onnx', '.pkl']
    if any(keyword in full_text for keyword in model_keywords):
        ai_analysis['has_model_files'] = True
    
    # æ£€æµ‹ç ”ç©¶è®ºæ–‡
    paper_keywords = ['paper', 'arxiv', 'research', 'publication', 'cite']
    if any(keyword in full_text for keyword in paper_keywords):
        ai_analysis['has_paper'] = True
    
    # å‰æ²¿æŠ€æœ¯è¯„åˆ† (0-25åˆ†)
    cutting_edge_keywords = {
        'gpt-4': 5, 'claude': 5, 'llama': 4, 'gemini': 4,
        'multimodal': 4, 'vision-language': 4,
        'agent': 3, 'autonomous': 3, 'reasoning': 3,
        'rag': 3, 'retrieval': 3, 'vector': 2,
        '2024': 3, 'sota': 4, 'state-of-art': 4
    }
    
    for keyword, score in cutting_edge_keywords.items():
        if keyword in full_text:
            ai_analysis['cutting_edge_score'] += score
    
    ai_analysis['cutting_edge_score'] = min(25, ai_analysis['cutting_edge_score'])
    
    # å®ç”¨æ€§è¯„åˆ† (0-20åˆ†)
    practical_keywords = {
        'api': 3, 'production': 4, 'deploy': 3, 'docker': 2,
        'web': 2, 'app': 2, 'service': 3, 'tool': 2,
        'library': 2, 'framework': 3, 'sdk': 2
    }
    
    for keyword, score in practical_keywords.items():
        if keyword in full_text:
            ai_analysis['practical_score'] += score
    
    ai_analysis['practical_score'] = min(20, ai_analysis['practical_score'])
    
    return ai_analysis

def calculate_comprehensive_scores(comprehensive_data):
    """è®¡ç®—ç»¼åˆè¯„åˆ†"""
    
    repo_data = comprehensive_data['basic_info']
    contributors = comprehensive_data['contributors']
    commits = comprehensive_data['commits']
    prs = comprehensive_data['pull_requests']
    issues = comprehensive_data['issues']
    releases = comprehensive_data['releases']
    languages = comprehensive_data['languages']
    content = comprehensive_data['content']
    ai_features = comprehensive_data['ai_features']
    
    # åŸºç¡€æ•°æ®
    stars = repo_data.get('stargazers_count', 0)
    forks = repo_data.get('forks_count', 0)
    watchers = repo_data.get('watchers_count', 0)
    
    # è®¡ç®—å„é¡¹è¯„åˆ†
    scores = {}
    
    # 1. è´¨é‡è¯„åˆ† (0-50åˆ†)
    quality_score = 0
    
    # æ˜Ÿæ ‡è¯„åˆ† (0-15åˆ†)
    if stars >= 10000:
        quality_score += 15
    elif stars >= 5000:
        quality_score += 12
    elif stars >= 1000:
        quality_score += 10
    elif stars >= 500:
        quality_score += 8
    elif stars >= 100:
        quality_score += 6
    elif stars >= 20:
        quality_score += 3
    
    # åˆ†å‰è¯„åˆ† (0-10åˆ†)
    if forks >= 2000:
        quality_score += 10
    elif forks >= 500:
        quality_score += 8
    elif forks >= 100:
        quality_score += 6
    elif forks >= 20:
        quality_score += 4
    elif forks >= 5:
        quality_score += 2
    
    # ç¤¾åŒºè¯„åˆ† (0-10åˆ†)
    if contributors >= 50:
        quality_score += 5
    elif contributors >= 10:
        quality_score += 3
    elif contributors >= 3:
        quality_score += 2
    
    # æ–‡æ¡£è¯„åˆ† (0-10åˆ†)
    quality_score += content['documentation_score'] * 2 / 3  # è½¬æ¢ä¸º10åˆ†åˆ¶
    if content['has_readme']:
        quality_score += 2
    if content['has_wiki']:
        quality_score += 1
    
    # æ´»è·ƒåº¦è¯„åˆ† (0-5åˆ†)
    if commits['last_commit_date']:
        try:
            last_commit = datetime.fromisoformat(commits['last_commit_date'].replace('Z', '+00:00'))
            days_ago = (datetime.now(last_commit.tzinfo) - last_commit).days
            
            if days_ago <= 7:
                quality_score += 5
            elif days_ago <= 30:
                quality_score += 4
            elif days_ago <= 90:
                quality_score += 3
            elif days_ago <= 365:
                quality_score += 1
        except:
            pass
    
    scores['quality_score'] = min(50, quality_score)
    
    # 2. å½±å“åŠ›è¯„åˆ† (0-30åˆ†)
    impact_score = 0
    
    # åŸºäºå…³æ³¨è€…å’Œåˆ†å‰çš„å½±å“åŠ›
    impact_score += min(15, stars // 1000)  # æ¯1000æ˜Ÿæ ‡1åˆ†ï¼Œæœ€é«˜15åˆ†
    impact_score += min(10, watchers // 100)  # æ¯100å…³æ³¨è€…1åˆ†ï¼Œæœ€é«˜10åˆ†
    impact_score += min(5, forks // 200)  # æ¯200åˆ†å‰1åˆ†ï¼Œæœ€é«˜5åˆ†
    
    scores['impact_score'] = min(30, impact_score)
    
    # 3. åˆ›æ–°è¯„åˆ† (0-20åˆ†)
    innovation_score = ai_features['cutting_edge_score']  # å·²ç»æ˜¯0-25åˆ†ï¼Œå–20åˆ†ä¸Šé™
    scores['innovation_score'] = min(20, innovation_score)
    
    # 4. æ´»è·ƒåº¦è¯„åˆ† (0-10åˆ†)
    activity_score = 0
    
    # åŸºäºæœ€è¿‘æ¨é€æ—¶é—´
    pushed_at = repo_data.get('pushed_at')
    if pushed_at:
        try:
            pushed_date = datetime.fromisoformat(pushed_at.replace('Z', '+00:00'))
            days_since_push = (datetime.now(pushed_date.tzinfo) - pushed_date).days
            
            if days_since_push <= 7:
                activity_score = 10
            elif days_since_push <= 30:
                activity_score = 8
            elif days_since_push <= 90:
                activity_score = 6
            elif days_since_push <= 180:
                activity_score = 4
            elif days_since_push <= 365:
                activity_score = 2
            
            scores['days_since_pushed'] = days_since_push
        except:
            scores['days_since_pushed'] = 999
    else:
        scores['days_since_pushed'] = 999
    
    scores['activity_score'] = activity_score
    
    # 5. å…¶ä»–ä¸“é¡¹è¯„åˆ†
    scores['enterprise_score'] = min(20, ai_features['practical_score'])
    scores['production_ready_score'] = min(10, ai_features['practical_score'] // 2)
    scores['api_score'] = 10 if 'api' in repo_data.get('description', '').lower() else 0
    scores['community_health_score'] = min(10, (contributors * 2 + (10 - min(10, issues['open_issues']))))
    
    # è®¡ç®—æ¯”ç‡
    scores['fork_ratio'] = round(forks / max(1, stars), 3)
    
    # è®¡ç®—è¶‹åŠ¿è¯„åˆ† (ç®€åŒ–ç‰ˆ)
    trending_factors = [
        scores['activity_score'] / 10,  # æ´»è·ƒåº¦æƒé‡
        min(1, stars / 1000),  # å—æ¬¢è¿ç¨‹åº¦æƒé‡  
        min(1, ai_features['cutting_edge_score'] / 25)  # å‰æ²¿æ€§æƒé‡
    ]
    scores['trending_score'] = int(sum(trending_factors) * 10 / 3)
    
    return scores

def create_enhanced_repo_record(comprehensive_data):
    """åˆ›å»ºå¢å¼ºç‰ˆä»“åº“è®°å½•"""
    
    repo_data = comprehensive_data['basic_info']
    contributors = comprehensive_data['contributors']
    commits = comprehensive_data['commits']
    prs = comprehensive_data['pull_requests']
    issues = comprehensive_data['issues']
    releases = comprehensive_data['releases']
    languages = comprehensive_data['languages']
    content = comprehensive_data['content']
    ai_features = comprehensive_data['ai_features']
    
    # è®¡ç®—ç»¼åˆè¯„åˆ†
    scores = calculate_comprehensive_scores(comprehensive_data)
    
    # æ„å»ºå®Œæ•´è®°å½•
    record = {
        # åŸºç¡€ä¿¡æ¯ (ä¿æŒå…¼å®¹)
        'id': str(repo_data.get('id')),
        'name': repo_data.get('name'),
        'owner': repo_data.get('owner', {}).get('login'),
        'description': repo_data.get('description'),
        'url': repo_data.get('html_url'),
        'created_at': repo_data.get('created_at'),
        'updated_at': repo_data.get('updated_at'),
        'sync_time': datetime.now().isoformat(),
        
        # åŸºç¡€GitHubæŒ‡æ ‡
        'stars': repo_data.get('stargazers_count', 0),
        'forks': repo_data.get('forks_count', 0),
        'watchers': repo_data.get('watchers_count', 0),
        'open_issues': repo_data.get('open_issues_count', 0),
        'size_kb': repo_data.get('size', 0),
        'language': languages['primary_language'],
        'default_branch': repo_data.get('default_branch', 'main'),
        'is_fork': repo_data.get('fork', False),
        
        # æ—¶é—´å’Œæ´»è·ƒåº¦
        'pushed_at': repo_data.get('pushed_at'),
        'last_commit_date': commits['last_commit_date'],
        'days_since_pushed': scores['days_since_pushed'],
        'activity_score': scores['activity_score'],
        
        # ç¤¾åŒºå’Œåä½œ
        'contributors_count': contributors,
        'commits_count': commits['total_commits'],
        'pull_requests_count': prs['total_prs'],
        'issues_count': issues['total_issues'],
        'releases_count': releases,
        'fork_ratio': scores['fork_ratio'],
        
        # è´¨é‡å’Œæˆç†Ÿåº¦
        'license_type': repo_data.get('license', {}).get('key') if repo_data.get('license') else None,
        'has_readme': content['has_readme'],
        'has_wiki': content['has_wiki'],
        'has_pages': content['has_pages'],
        'has_issues': repo_data.get('has_issues', True),
        'has_projects': repo_data.get('has_projects', False),
        'has_discussions': repo_data.get('has_discussions', False),
        
        # AI/MLç‰¹å®š
        'ai_framework': ai_features['ai_framework'],
        'model_type': ai_features['model_type'],
        'has_model_files': ai_features['has_model_files'],
        'has_paper': ai_features['has_paper'],
        'cutting_edge_score': ai_features['cutting_edge_score'],
        'practical_score': ai_features['practical_score'],
        
        # å•†ä¸šåº”ç”¨ä»·å€¼
        'enterprise_score': scores['enterprise_score'],
        'production_ready_score': scores['production_ready_score'],
        'api_score': scores['api_score'],
        'documentation_score': content['documentation_score'],
        'community_health_score': scores['community_health_score'],
        
        # ç»¼åˆè¯„åˆ†
        'quality_score': scores['quality_score'],
        'impact_score': scores['impact_score'],
        'innovation_score': scores['innovation_score'],
        
        # æ ‡ç­¾å’Œåˆ†ç±»
        'topics': ','.join(repo_data.get('topics', [])),
        'tech_stack': languages['tech_stack'],
        
        # è¶‹åŠ¿åˆ†æ
        'trending_score': scores['trending_score'],
        
        # å…¼å®¹å­—æ®µ (ä¿æŒç°æœ‰ç³»ç»Ÿæ­£å¸¸è¿è¡Œ)
        'relevance_score': min(10, scores['quality_score'] // 5),  # è½¬æ¢ä¸º10åˆ†åˆ¶
        'category': classify_repo_category(ai_features, repo_data),
        'tags': generate_smart_tags(ai_features, languages, repo_data),
        'summary': generate_repo_summary(repo_data, ai_features),
        
        # å…ƒæ•°æ®
        'data_version': 'v2.0',
        'last_analyzed_at': datetime.now().isoformat(),
        'analysis_status': 'completed',
        'data_completeness_score': calculate_data_completeness(comprehensive_data)
    }
    
    return record

def classify_repo_category(ai_features, repo_data):
    """æ™ºèƒ½åˆ†ç±»ä»“åº“ç±»åˆ«"""
    
    model_type = ai_features['model_type']
    ai_framework = ai_features['ai_framework']
    
    if model_type == 'llm':
        return 'LLMæœåŠ¡ä¸å·¥å…·'
    elif model_type == 'cv':
        return 'è®¡ç®—æœºè§†è§‰'
    elif model_type == 'rag':
        return 'RAGæŠ€æœ¯'
    elif model_type == 'agent':
        return 'AIä»£ç†ç³»ç»Ÿ'
    elif model_type == 'generative':
        return 'ç”Ÿæˆå¼AI'
    elif ai_framework in ['pytorch', 'tensorflow']:
        return 'æœºå™¨å­¦ä¹ æ¡†æ¶'
    else:
        return 'é€šç”¨AIå·¥å…·'

def generate_smart_tags(ai_features, languages, repo_data):
    """ç”Ÿæˆæ™ºèƒ½æ ‡ç­¾"""
    
    tags = []
    
    # AIæ¡†æ¶æ ‡ç­¾
    if ai_features['ai_framework'] != 'unknown':
        tags.append(ai_features['ai_framework'].title())
    
    # æ¨¡å‹ç±»å‹æ ‡ç­¾
    if ai_features['model_type'] != 'unknown':
        tags.append(ai_features['model_type'].upper())
    
    # æŠ€æœ¯æ ˆæ ‡ç­¾
    primary_lang = languages['primary_language']
    if primary_lang != 'Unknown':
        tags.append(primary_lang)
    
    # ç‰¹å¾æ ‡ç­¾
    if ai_features['has_model_files']:
        tags.append('Pre-trained Models')
    
    if ai_features['has_paper']:
        tags.append('Research')
    
    if ai_features['cutting_edge_score'] > 15:
        tags.append('Cutting-edge')
    
    if ai_features['practical_score'] > 10:
        tags.append('Production-ready')
    
    return ', '.join(tags[:6])  # é™åˆ¶æ ‡ç­¾æ•°é‡

def generate_repo_summary(repo_data, ai_features):
    """ç”Ÿæˆä»“åº“æ‘˜è¦"""
    
    name = repo_data.get('name', '')
    description = repo_data.get('description', '')
    
    if description:
        # æˆªå–æè¿°çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
        summary = description[:50] + '...' if len(description) > 50 else description
    else:
        # åŸºäºåˆ†æç»“æœç”Ÿæˆæ‘˜è¦
        framework = ai_features['ai_framework']
        model_type = ai_features['model_type']
        
        if framework != 'unknown' and model_type != 'unknown':
            summary = f"{name} - {framework.title()}åŸºç¡€çš„{model_type.upper()}é¡¹ç›®"
        elif framework != 'unknown':
            summary = f"{name} - åŸºäº{framework.title()}çš„AIé¡¹ç›®"
        else:
            summary = f"{name} - AI/MLç›¸å…³é¡¹ç›®"
    
    return summary

def calculate_data_completeness(comprehensive_data):
    """è®¡ç®—æ•°æ®å®Œæ•´æ€§è¯„åˆ†"""
    
    total_fields = 20  # å…³é”®å­—æ®µæ€»æ•°
    completed_fields = 0
    
    repo_data = comprehensive_data['basic_info']
    
    # æ£€æŸ¥å…³é”®å­—æ®µå®Œæ•´æ€§
    key_fields = [
        'name', 'owner', 'description', 'stargazers_count', 'forks_count',
        'watchers_count', 'language', 'created_at', 'updated_at', 'pushed_at'
    ]
    
    for field in key_fields:
        if field in repo_data and repo_data[field] is not None:
            completed_fields += 1
    
    # æ£€æŸ¥æ‰©å±•æ•°æ®å®Œæ•´æ€§
    if comprehensive_data['contributors'] > 0:
        completed_fields += 2
    
    if comprehensive_data['commits']['total_commits'] > 0:
        completed_fields += 2
    
    if comprehensive_data['content']['has_readme']:
        completed_fields += 2
    
    if comprehensive_data['ai_features']['ai_framework'] != 'unknown':
        completed_fields += 2
    
    if comprehensive_data['languages']['primary_language'] != 'Unknown':
        completed_fields += 2
    
    return min(100, int(completed_fields / total_fields * 100))

def save_enhanced_repo_to_database(record):
    """ä¿å­˜å¢å¼ºè®°å½•åˆ°æ•°æ®åº“"""
    
    try:
        # æ„å»ºSQLè¯­å¥ (åŒ…å«æ‰€æœ‰æ–°å­—æ®µ)
        sql = """
        INSERT INTO repos (
            id, name, owner, description, url, created_at, updated_at, sync_time,
            stars, forks, watchers, open_issues, size_kb, language, default_branch, is_fork,
            pushed_at, last_commit_date, days_since_pushed, activity_score,
            contributors_count, commits_count, pull_requests_count, issues_count, releases_count, fork_ratio,
            license_type, has_readme, has_wiki, has_pages, has_issues, has_projects, has_discussions,
            ai_framework, model_type, has_model_files, has_paper, cutting_edge_score, practical_score,
            enterprise_score, production_ready_score, api_score, documentation_score, community_health_score,
            quality_score, impact_score, innovation_score,
            topics, tech_stack, trending_score,
            relevance_score, category, tags, summary,
            data_version, last_analyzed_at, analysis_status, data_completeness_score
        ) VALUES (
            ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
        )
        ON CONFLICT(id) DO UPDATE SET
            stars=excluded.stars, forks=excluded.forks, watchers=excluded.watchers,
            pushed_at=excluded.pushed_at, last_commit_date=excluded.last_commit_date,
            days_since_pushed=excluded.days_since_pushed, activity_score=excluded.activity_score,
            contributors_count=excluded.contributors_count, commits_count=excluded.commits_count,
            pull_requests_count=excluded.pull_requests_count, issues_count=excluded.issues_count,
            quality_score=excluded.quality_score, impact_score=excluded.impact_score,
            innovation_score=excluded.innovation_score, trending_score=excluded.trending_score,
            cutting_edge_score=excluded.cutting_edge_score, practical_score=excluded.practical_score,
            data_version=excluded.data_version, last_analyzed_at=excluded.last_analyzed_at,
            analysis_status=excluded.analysis_status, sync_time=excluded.sync_time
        """
        
        # æ„å»ºå‚æ•°åˆ—è¡¨
        params = [
            record['id'], record['name'], record['owner'], record['description'], 
            record['url'], record['created_at'], record['updated_at'], record['sync_time'],
            record['stars'], record['forks'], record['watchers'], record['open_issues'],
            record['size_kb'], record['language'], record['default_branch'], record['is_fork'],
            record['pushed_at'], record['last_commit_date'], record['days_since_pushed'], record['activity_score'],
            record['contributors_count'], record['commits_count'], record['pull_requests_count'], 
            record['issues_count'], record['releases_count'], record['fork_ratio'],
            record['license_type'], record['has_readme'], record['has_wiki'], record['has_pages'],
            record['has_issues'], record['has_projects'], record['has_discussions'],
            record['ai_framework'], record['model_type'], record['has_model_files'], record['has_paper'],
            record['cutting_edge_score'], record['practical_score'],
            record['enterprise_score'], record['production_ready_score'], record['api_score'],
            record['documentation_score'], record['community_health_score'],
            record['quality_score'], record['impact_score'], record['innovation_score'],
            record['topics'], record['tech_stack'], record['trending_score'],
            record['relevance_score'], record['category'], record['tags'], record['summary'],
            record['data_version'], record['last_analyzed_at'], record['analysis_status'], 
            record['data_completeness_score']
        ]
        
        # æ‰§è¡Œæ•°æ®åº“æ“ä½œ
        response = cloudflare_client.d1.database.query(
            database_id=D1_DATABASE_ID,
            account_id=CLOUDFLARE_ACCOUNT_ID,
            sql=sql,
            params=params
        )
        
        if response.success:
            print(f"âœ… æˆåŠŸä¿å­˜ {record['owner']}/{record['name']} çš„å¢å¼ºæ•°æ®")
            
            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            print(f"   ğŸ“Š è´¨é‡è¯„åˆ†: {record['quality_score']}/50")
            print(f"   ğŸ¯ å½±å“åŠ›è¯„åˆ†: {record['impact_score']}/30") 
            print(f"   ğŸ’¡ åˆ›æ–°è¯„åˆ†: {record['innovation_score']}/20")
            print(f"   âš¡ æ´»è·ƒåº¦è¯„åˆ†: {record['activity_score']}/10")
            print(f"   ğŸ¤– AIæ¡†æ¶: {record['ai_framework']}")
            print(f"   ğŸ·ï¸ æ¨¡å‹ç±»å‹: {record['model_type']}")
            
            return True
        else:
            print(f"âŒ ä¿å­˜å¤±è´¥: {response}")
            return False
            
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æ“ä½œé”™è¯¯: {e}")
        return False

def main_enhanced_collection():
    """ä¸»è¦çš„å¢å¼ºæ•°æ®æ”¶é›†æµç¨‹"""
    
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆGitHub AIä»“åº“æ•°æ®æ”¶é›†ç³»ç»Ÿ v2.0")
    print("=" * 70)
    
    # æµ‹è¯•é¡¹ç›®åˆ—è¡¨ (å¯æ‰©å±•)
    test_repos = [
        ("huggingface", "transformers"),
        ("pytorch", "pytorch"),
        ("tensorflow", "tensorflow"),
        ("openai", "openai-python"),
        ("langchain-ai", "langchain")
    ]
    
    successful_collections = 0
    
    for owner, repo_name in test_repos:
        print(f"\nğŸ“Š æ­£åœ¨å¤„ç† {owner}/{repo_name}...")
        print("-" * 50)
        
        # è·å–å®Œæ•´æ•°æ®
        comprehensive_data = fetch_comprehensive_repo_data(owner, repo_name)
        
        if comprehensive_data:
            # åˆ›å»ºå¢å¼ºè®°å½•
            record = create_enhanced_repo_record(comprehensive_data)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if save_enhanced_repo_to_database(record):
                successful_collections += 1
        
        # APIé€Ÿç‡é™åˆ¶å»¶è¿Ÿ
        print("â±ï¸ ç­‰å¾…2ç§’...")
        time.sleep(2)
    
    print(f"\nğŸ‰ å¢å¼ºæ•°æ®æ”¶é›†å®Œæˆ!")
    print("=" * 70)
    print(f"âœ… æˆåŠŸæ”¶é›†: {successful_collections}/{len(test_repos)} ä¸ªé¡¹ç›®")
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°Cloudflare D1æ•°æ®åº“")
    print(f"ğŸ“Š åŒ…å«59ä¸ªæ–°å¢å­—æ®µçš„å®Œæ•´æ•°æ®")

if __name__ == "__main__":
    main_enhanced_collection()
