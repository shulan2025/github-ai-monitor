#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæœç´¢é…ç½® - ä¸ºæé«˜æœ‰æ•ˆæ•°æ®æ”¶é›†ç‡è®¾è®¡
ç›®æ ‡ï¼šä»20%æœ‰æ•ˆç‡æå‡åˆ°æ¯å¤©200æ¡æœ‰æ•ˆæ•°æ®
"""

# ================================
# ğŸ¯ å¢å¼ºæœç´¢é…ç½®
# ================================

# åŸºç¡€é…ç½®ï¼šå¤šè½®æ¬¡æœç´¢ç­–ç•¥
ENHANCED_SEARCH_CONFIG = {
    # GitHub APIé™åˆ¶ï¼šæ¯æ¬¡æœ€å¤š100æ¡
    "per_page": 100,
    
    # å¤šè½®æœç´¢é…ç½®
    "search_rounds": 10,  # æ‰§è¡Œ10è½®æœç´¢ï¼Œç†è®ºä¸Šå¯è·å–1000æ¡å€™é€‰
    
    # æ—¶é—´çª—å£åˆ†å‰²
    "time_splits": 3,     # å°†30å¤©åˆ†ä¸º3ä¸ª10å¤©çª—å£
    
    # æ˜Ÿæ ‡è¦æ±‚åˆ†å±‚ï¼ˆæ›´ç°å®çš„è®¾ç½®ï¼‰
    "star_tiers": [
        {"min": 100, "days": 30},  # é«˜è´¨é‡ï¼š30å¤©å†…100+æ˜Ÿæ ‡
        {"min": 50, "days": 60},   # ä¸­è´¨é‡ï¼š60å¤©å†…50+æ˜Ÿæ ‡  
        {"min": 20, "days": 90},   # å¹¿è¦†ç›–ï¼š90å¤©å†…20+æ˜Ÿæ ‡
        {"min": 10, "days": 180},  # å‘ç°æ–°é¡¹ç›®ï¼š180å¤©å†…10+æ˜Ÿæ ‡
    ]
}

# ================================
# ğŸ” å¤šç»´åº¦æœç´¢å…³é”®è¯
# ================================

# æ ¸å¿ƒæŠ€æœ¯æ ˆå…³é”®è¯ï¼ˆåˆ†ç»„æœç´¢ï¼‰
TECH_KEYWORD_GROUPS = {
    "llm_core": ["LLM", "large-language-model", "transformer", "GPT"],
    "llm_training": ["fine-tuning", "PEFT", "LoRA", "QLoRA", "instruction-tuning"],
    "llm_inference": ["inference", "serving", "vllm", "ollama", "llama"],
    
    "rag_tech": ["RAG", "retrieval-augmented", "vector-database", "embedding"],
    "rag_tools": ["chromadb", "pinecone", "weaviate", "qdrant", "milvus"],
    
    "diffusion_models": ["diffusion", "stable-diffusion", "DDPM", "DDIM"],
    "image_gen": ["text-to-image", "image-generation", "DALL-E", "midjourney"],
    
    "ml_frameworks": ["pytorch", "tensorflow", "jax", "scikit-learn"],
    "ml_algorithms": ["deep-learning", "neural-network", "machine-learning"],
    
    "cv_detection": ["object-detection", "YOLO", "RCNN", "detection"],
    "cv_recognition": ["image-classification", "face-recognition", "OCR"],
    "cv_segmentation": ["segmentation", "semantic-segmentation", "instance-segmentation"],
    
    "data_processing": ["data-science", "pandas", "numpy", "jupyter"],
    "data_analysis": ["data-analysis", "visualization", "analytics", "statistics"],
    
    "ai_agents": ["agent", "autonomous", "multi-agent", "langchain"],
    "ai_tools": ["chatbot", "assistant", "automation", "workflow"]
}

# çƒ­é—¨å¼€æºé¡¹ç›®å…³é”®è¯
TRENDING_KEYWORDS = [
    # 2024å¹´çƒ­é—¨æŠ€æœ¯
    "multimodal", "vision-language", "CLIP", "BLIP",
    "code-generation", "coding-assistant", "github-copilot",
    "speech-to-text", "text-to-speech", "whisper",
    "robotics", "embodied-ai", "simulation",
    "federated-learning", "privacy-preserving",
    "edge-ai", "mobile-ai", "quantization",
    "mlops", "model-deployment", "monitoring"
]

# ç¼–ç¨‹è¯­è¨€ç»„åˆï¼ˆæé«˜åŒ¹é…åº¦ï¼‰
LANGUAGE_COMBINATIONS = [
    ["Python"],
    ["JavaScript", "TypeScript"],  
    ["Rust"],
    ["Go"],
    ["C++"],
    ["Julia", "R"]
]

# ================================
# ğŸ¯ æ™ºèƒ½æœç´¢ç­–ç•¥
# ================================

def generate_search_queries():
    """ç”Ÿæˆå¤šæ ·åŒ–çš„æœç´¢æŸ¥è¯¢"""
    queries = []
    
    # ç­–ç•¥1ï¼šæŠ€æœ¯æ ˆç»„åˆæœç´¢
    for group_name, keywords in TECH_KEYWORD_GROUPS.items():
        for i in range(0, len(keywords), 2):  # æ¯2ä¸ªå…³é”®è¯ä¸€ç»„
            keyword_pair = keywords[i:i+2]
            query_terms = " OR ".join(keyword_pair)
            queries.append({
                "name": f"tech_{group_name}_{i//2}",
                "terms": query_terms,
                "focus": group_name
            })
    
    # ç­–ç•¥2ï¼šçƒ­é—¨æŠ€æœ¯æœç´¢
    for i in range(0, len(TRENDING_KEYWORDS), 3):  # æ¯3ä¸ªå…³é”®è¯ä¸€ç»„
        keyword_group = TRENDING_KEYWORDS[i:i+3]
        query_terms = " OR ".join(keyword_group)
        queries.append({
            "name": f"trending_{i//3}",
            "terms": query_terms,
            "focus": "trending"
        })
    
    # ç­–ç•¥3ï¼šè¯­è¨€ç‰¹å®šæœç´¢
    for lang_group in LANGUAGE_COMBINATIONS:
        for tech_group, tech_keywords in list(TECH_KEYWORD_GROUPS.items())[:3]:  # åªç”¨å‰3ä¸ªæŠ€æœ¯ç»„
            lang_filter = " OR ".join([f"language:{lang}" for lang in lang_group])
            tech_filter = " OR ".join(tech_keywords[:2])  # åªç”¨å‰2ä¸ªå…³é”®è¯
            queries.append({
                "name": f"lang_{'-'.join(lang_group).lower()}_{tech_group}",
                "terms": f"({tech_filter}) {lang_filter}",
                "focus": f"language_{lang_group[0]}"
            })
    
    return queries

# ================================
# ğŸ“Š æœç´¢æ’åºç­–ç•¥
# ================================

SORT_STRATEGIES = [
    {"sort": "stars", "order": "desc"},      # æŒ‰æ˜Ÿæ ‡é™åº
    {"sort": "updated", "order": "desc"},    # æŒ‰æ›´æ–°æ—¶é—´é™åº
    {"sort": "created", "order": "desc"},    # æŒ‰åˆ›å»ºæ—¶é—´é™åº
]

# ================================
# ğŸ”„ æ—¶é—´çª—å£ç­–ç•¥
# ================================

def get_time_windows():
    """è·å–æ—¶é—´çª—å£åˆ—è¡¨ï¼Œæ‰©å¤§æ—¶é—´èŒƒå›´ä»¥è·å–æ›´å¤šé¡¹ç›®"""
    from datetime import datetime, timedelta
    
    windows = []
    
    # æœ€è¿‘30å¤©ï¼ˆé«˜è´¨é‡é¡¹ç›®ï¼‰
    end = datetime.now()
    start = end - timedelta(days=30)
    windows.append({
        "name": "recent_month",
        "range": f"{start.strftime('%Y-%m-%d')}..{end.strftime('%Y-%m-%d')}",
        "priority": "high",
        "min_stars": 100
    })
    
    # æœ€è¿‘60å¤©ï¼ˆä¸­ç­‰è´¨é‡ï¼‰
    end = datetime.now() - timedelta(days=30)
    start = end - timedelta(days=60)
    windows.append({
        "name": "recent_2months",
        "range": f"{start.strftime('%Y-%m-%d')}..{end.strftime('%Y-%m-%d')}",
        "priority": "medium",
        "min_stars": 50
    })
    
    # æœ€è¿‘90å¤©ï¼ˆå¹¿æ³›è¦†ç›–ï¼‰
    end = datetime.now() - timedelta(days=60)
    start = end - timedelta(days=90)
    windows.append({
        "name": "recent_3months",
        "range": f"{start.strftime('%Y-%m-%d')}..{end.strftime('%Y-%m-%d')}",
        "priority": "medium",
        "min_stars": 20
    })
    
    # æœ€è¿‘180å¤©ï¼ˆå‘ç°æ–°å…´é¡¹ç›®ï¼‰
    end = datetime.now() - timedelta(days=90)
    start = end - timedelta(days=180)
    windows.append({
        "name": "recent_6months",
        "range": f"{start.strftime('%Y-%m-%d')}..{end.strftime('%Y-%m-%d')}",
        "priority": "low",
        "min_stars": 10
    })
    
    return windows

# ================================
# ğŸ¯ è¿‡æ»¤ä¼˜åŒ–é…ç½®
# ================================

# é™ä½è¿‡æ»¤é˜ˆå€¼ä»¥è·å–æ›´å¤šå€™é€‰
ENHANCED_FILTER_CONFIG = {
    "ai_relevance_threshold": 1,  # ä»2é™åˆ°1ï¼Œè·å–æ›´å¤šå€™é€‰
    "min_description_length": 20,  # è‡³å°‘20å­—ç¬¦æè¿°
    "exclude_forks": True,         # æ’é™¤Forké¡¹ç›®
    "exclude_archived": True,      # æ’é™¤å·²å½’æ¡£é¡¹ç›®
    "min_repo_age_days": 1,       # è‡³å°‘å­˜åœ¨1å¤©
}

# é‡æ–°å®šä¹‰é«˜ä»·å€¼å…³é”®è¯ï¼ˆæ›´å…¨é¢ï¼‰
HIGH_VALUE_KEYWORDS = [
    # AIæ ¸å¿ƒ
    "artificial-intelligence", "machine-learning", "deep-learning",
    "neural-network", "transformer", "attention", "bert", "gpt",
    
    # å…·ä½“åº”ç”¨
    "computer-vision", "natural-language-processing", "nlp", 
    "speech-recognition", "image-recognition", "object-detection",
    
    # å·¥å…·æ¡†æ¶
    "pytorch", "tensorflow", "scikit-learn", "keras", "jax",
    "huggingface", "transformers", "diffusers", "langchain",
    
    # æ–°å…´æŠ€æœ¯
    "multimodal", "foundation-model", "large-language-model",
    "diffusion-model", "generative-ai", "federated-learning"
]

# ================================
# ğŸ“ˆ æ‰§è¡Œç­–ç•¥é…ç½®
# ================================

EXECUTION_STRATEGY = {
    # æœç´¢è½®æ¬¡åˆ†é…
    "primary_searches": 15,    # ä¸»è¦æœç´¢15æ¬¡
    "secondary_searches": 10,  # æ¬¡è¦æœç´¢10æ¬¡
    "exploratory_searches": 5, # æ¢ç´¢æ€§æœç´¢5æ¬¡
    
    # APIè°ƒç”¨é—´éš”ï¼ˆéµå®ˆGitHubé™åˆ¶ï¼‰
    "request_delay": 1.0,      # æ¯æ¬¡è¯·æ±‚é—´éš”1ç§’
    "batch_delay": 10.0,       # æ¯æ‰¹æ¬¡é—´éš”10ç§’
    
    # ç›®æ ‡è®¾ç½®
    "target_candidates": 1000, # ç›®æ ‡å€™é€‰é¡¹ç›®æ•°
    "target_valid": 200,       # ç›®æ ‡æœ‰æ•ˆé¡¹ç›®æ•°
    "expected_efficiency": 0.2 # é¢„æœŸ20%æœ‰æ•ˆç‡
}

# ================================
# ğŸ’¡ ä½¿ç”¨è¯´æ˜
# ================================
"""
ğŸ¯ å¢å¼ºæœç´¢ç­–ç•¥è¯´æ˜ï¼š

1. å¤šè½®æœç´¢ï¼šæ‰§è¡Œ30è½®ä¸åŒçš„æœç´¢æŸ¥è¯¢
2. æ—¶é—´åˆ†å‰²ï¼šè¦†ç›–ä¸åŒæ—¶é—´çª—å£
3. æŠ€æœ¯åˆ†ç»„ï¼šæŒ‰AIæŠ€æœ¯æ ˆåˆ†ç»„æœç´¢
4. è¯­è¨€è¿‡æ»¤ï¼šé’ˆå¯¹ç‰¹å®šç¼–ç¨‹è¯­è¨€
5. æ’åºå¤šæ ·ï¼šä½¿ç”¨ä¸åŒæ’åºç­–ç•¥

é¢„æœŸæ•ˆæœï¼š
- å€™é€‰é¡¹ç›®ï¼š1000+ 
- æœ‰æ•ˆé¡¹ç›®ï¼š200+
- æœ‰æ•ˆç‡ï¼š20%+
- é‡å¤ç‡ï¼š<10%ï¼ˆé€šè¿‡æ•°æ®åº“å»é‡ï¼‰

GitHub APIé™åˆ¶ï¼š
- æœç´¢APIï¼š1000æ¬¡/å°æ—¶
- æˆ‘ä»¬ä½¿ç”¨ï¼š30æ¬¡/å¤©
- å®‰å…¨ä½™é‡ï¼šå……è¶³
"""
