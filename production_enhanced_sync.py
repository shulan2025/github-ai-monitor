#!/usr/bin/env python3
"""
ç”Ÿäº§çº§å¢å¼ºç‰ˆGitHub AIä»“åº“æ”¶é›†è„šæœ¬
åŸºäºæµ‹è¯•ç»“æœä¼˜åŒ–ï¼Œç›®æ ‡æ¯å¤©200æ¡æœ‰æ•ˆæ•°æ®
"""

import os
import requests
import json
import time
from datetime import datetime, timedelta
from cloudflare import Cloudflare
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# === é…ç½®éƒ¨åˆ† ===
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
CLOUDFLARE_API_TOKEN = os.environ.get("CLOUDFLARE_API_TOKEN")
CLOUDFLARE_ACCOUNT_ID = os.environ.get("CLOUDFLARE_ACCOUNT_ID")
D1_DATABASE_ID = os.environ.get("D1_DATABASE_ID")

if not all([GITHUB_TOKEN, CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID, D1_DATABASE_ID]):
    raise ValueError("ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚è¯·ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç¯å¢ƒå˜é‡éƒ½å·²æ­£ç¡®é…ç½®ã€‚")

# åˆå§‹åŒ–å®¢æˆ·ç«¯
cloudflare_client = Cloudflare(api_token=CLOUDFLARE_API_TOKEN)

# GitHub APIè®¾ç½®
github_url = "https://api.github.com/search/repositories"
github_headers = {
    "Authorization": f"token {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

# === åŸºäºæµ‹è¯•ç»“æœçš„ä¼˜åŒ–æœç´¢ç­–ç•¥ ===

def get_production_search_queries():
    """ç”Ÿäº§çº§æœç´¢æŸ¥è¯¢é…ç½®"""
    
    # åŸºäºæµ‹è¯•ç»“æœï¼Œè¿™äº›å…³é”®è¯æœ‰å¾ˆå¥½çš„æ•ˆæœ
    high_yield_keywords = [
        "LLM",                    # 520ä¸ªé¡¹ç›®
        "AI",                     # 1276ä¸ªé¡¹ç›®  
        "artificial-intelligence", # 49ä¸ªé¡¹ç›®
        "diffusion",              # 103ä¸ªé¡¹ç›®
        "GPT",                    # 71ä¸ªé¡¹ç›®
        "pytorch",                # 57ä¸ªé¡¹ç›®
        "machine-learning",       # 56ä¸ªé¡¹ç›®
        "deep-learning",          # 52ä¸ªé¡¹ç›®
        "transformer",            # 36ä¸ªé¡¹ç›®
        "computer-vision",        # 24ä¸ªé¡¹ç›®
    ]
    
    # æŠ€æœ¯ç»„åˆå…³é”®è¯
    tech_combinations = [
        "RAG OR retrieval-augmented",
        "stable-diffusion OR text-to-image",
        "chatbot OR chat OR conversation",
        "neural-network OR CNN OR RNN",
        "object-detection OR YOLO",
        "natural-language-processing OR NLP",
        "reinforcement-learning OR RL",
        "generative-ai OR generation",
        "multimodal OR vision-language",
        "fine-tuning OR PEFT OR LoRA"
    ]
    
    # çƒ­é—¨æ¡†æ¶å’Œå·¥å…·
    framework_keywords = [
        "huggingface OR transformers",
        "langchain OR llama",
        "tensorflow OR keras",
        "scikit-learn OR sklearn",
        "opencv OR cv2",
        "jupyter OR notebook",
        "gradio OR streamlit",
        "docker OR deployment"
    ]
    
    return high_yield_keywords + tech_combinations + framework_keywords

def get_production_time_windows():
    """ç”Ÿäº§çº§æ—¶é—´çª—å£é…ç½®"""
    windows = []
    
    # 30å¤©çª—å£ï¼Œä¸åŒæ˜Ÿæ ‡è¦æ±‚
    end = datetime.now()
    
    # é«˜è´¨é‡é¡¹ç›® (30å¤©, 100+æ˜Ÿ)
    start = end - timedelta(days=30)
    windows.append({
        "name": "high_quality_30d",
        "range": f"{start.strftime('%Y-%m-%d')}..{end.strftime('%Y-%m-%d')}",
        "min_stars": 100,
        "priority": 1
    })
    
    # ä¸­è´¨é‡é¡¹ç›® (30å¤©, 50+æ˜Ÿ)
    windows.append({
        "name": "medium_quality_30d", 
        "range": f"{start.strftime('%Y-%m-%d')}..{end.strftime('%Y-%m-%d')}",
        "min_stars": 50,
        "priority": 2
    })
    
    # å¹¿æ³›è¦†ç›– (30å¤©, 20+æ˜Ÿ)
    windows.append({
        "name": "broad_coverage_30d",
        "range": f"{start.strftime('%Y-%m-%d')}..{end.strftime('%Y-%m-%d')}",
        "min_stars": 20,
        "priority": 3
    })
    
    # 90å¤©çª—å£ï¼Œå‘ç°æ›´å¤šé¡¹ç›®
    start_90 = end - timedelta(days=90)
    windows.append({
        "name": "discovery_90d",
        "range": f"{start_90.strftime('%Y-%m-%d')}..{end.strftime('%Y-%m-%d')}",
        "min_stars": 20,
        "priority": 4
    })
    
    return windows

def execute_github_search(keyword, time_window, sort_method="stars"):
    """æ‰§è¡ŒGitHubæœç´¢"""
    
    query = f"{keyword} stars:>{time_window['min_stars']} created:{time_window['range']} is:public archived:false"
    
    params = {
        "q": query,
        "sort": sort_method,
        "order": "desc", 
        "per_page": 100
    }
    
    try:
        print(f"ğŸ” æœç´¢: {keyword[:20]:20} | çª—å£: {time_window['name']:15} | æœ€å°æ˜Ÿæ ‡: {time_window['min_stars']:3d}")
        
        response = requests.get(github_url, headers=github_headers, params=params)
        response.raise_for_status()
        
        data = response.json()
        repos = data.get("items", [])
        total_count = data.get("total_count", 0)
        
        print(f"   ğŸ“Š æ‰¾åˆ° {total_count} ä¸ªé¡¹ç›®ï¼Œè·å–å‰ {len(repos)} ä¸ª")
        
        return repos
        
    except requests.exceptions.RequestException as e:
        print(f"   âŒ æœç´¢å¤±è´¥: {e}")
        return []

def calculate_ai_relevance_score(repo):
    """è®¡ç®—AIç›¸å…³æ€§è¯„åˆ† (ä¼˜åŒ–ç‰ˆ)"""
    name = repo.get("name", "").lower()
    description = repo.get("description", "").lower() if repo.get("description") else ""
    full_text = f"{name} {description}"
    
    score = 0
    
    # é«˜ä»·å€¼AIå…³é”®è¯ (3åˆ†æ¯ä¸ª)
    high_value = ["llm", "gpt", "transformer", "diffusion", "neural", "deep-learning"]
    score += sum(3 for keyword in high_value if keyword in full_text)
    
    # ä¸­ä»·å€¼å…³é”®è¯ (2åˆ†æ¯ä¸ª)
    medium_value = ["machine-learning", "ai", "computer-vision", "nlp", "pytorch", "tensorflow"]
    score += sum(2 for keyword in medium_value if keyword in full_text)
    
    # æŠ€æœ¯å·¥å…· (1åˆ†æ¯ä¸ª)
    tools = ["python", "api", "model", "training", "inference", "framework"]
    score += sum(1 for keyword in tools if keyword in full_text)
    
    # è´¨é‡åŠ åˆ†
    stars = repo.get("stargazers_count", 0)
    if stars >= 1000:
        score += 3
    elif stars >= 500:
        score += 2
    elif stars >= 100:
        score += 1
    
    # æ´»è·ƒåº¦åŠ åˆ†
    updated_at = repo.get("updated_at", "")
    if updated_at:
        try:
            updated = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
            days_ago = (datetime.now().replace(tzinfo=updated.tzinfo) - updated).days
            if days_ago <= 30:
                score += 2
            elif days_ago <= 90:
                score += 1
        except:
            pass
    
    # è´Ÿé¢å…³é”®è¯æ‰£åˆ†
    negative = ["tutorial", "example", "demo", "course", "learning", "study", "homework"]
    score -= sum(1 for keyword in negative if keyword in full_text)
    
    return min(10, max(0, score))

def filter_and_process_repos(repos):
    """è¿‡æ»¤å’Œå¤„ç†ä»“åº“æ•°æ®"""
    processed = []
    
    for repo in repos:
        try:
            # åŸºç¡€è¿‡æ»¤
            if repo.get("fork", False):
                continue
                
            if repo.get("archived", False):
                continue
                
            description = repo.get("description", "")
            if len(description) < 10:  # è‡³å°‘10å­—ç¬¦æè¿°
                continue
            
            # AIç›¸å…³æ€§è¯„åˆ†
            relevance_score = calculate_ai_relevance_score(repo)
            
            if relevance_score < 2:  # è‡³å°‘2åˆ†æ‰ä¿ç•™
                continue
            
            # åˆ†ç±»
            category = categorize_project(repo.get("name", ""), description)
            
            # æå–æ ‡ç­¾
            tags = extract_tags(repo.get("name", ""), description)
            
            # ç”Ÿæˆæ‘˜è¦
            summary = f"{repo.get('name', '')} - {description.split('.')[0]}"[:100]
            
            processed_repo = {
                "id": str(repo.get("id")),
                "name": repo.get("name"),
                "owner": repo.get("owner", {}).get("login"),
                "stars": repo.get("stargazers_count", 0),
                "forks": repo.get("forks_count", 0),
                "description": description,
                "url": repo.get("html_url"),
                "created_at": repo.get("created_at"),
                "updated_at": repo.get("pushed_at"),
                "category": category,
                "tags": tags,
                "summary": summary,
                "relevance_score": relevance_score
            }
            
            processed.append(processed_repo)
            
        except Exception as e:
            print(f"âŒ å¤„ç†é¡¹ç›®æ—¶å‡ºé”™: {e}")
            continue
    
    return processed

def categorize_project(name, description):
    """é¡¹ç›®åˆ†ç±»"""
    text = f"{name} {description}".lower()
    
    if any(k in text for k in ["llm", "large language", "gpt", "chatgpt", "language model"]):
        if any(k in text for k in ["api", "server", "serving", "inference"]):
            return "LLMæœåŠ¡ä¸å·¥å…·"
        elif any(k in text for k in ["chat", "assistant", "bot", "conversation"]):
            return "LLMåº”ç”¨"
        else:
            return "LLMç ”ç©¶"
    elif any(k in text for k in ["rag", "retrieval", "vector", "embedding"]):
        return "RAGæŠ€æœ¯"
    elif any(k in text for k in ["diffusion", "stable-diffusion", "text-to-image", "generation"]):
        return "ç”Ÿæˆå¼AI"
    elif any(k in text for k in ["computer vision", "object detection", "yolo", "opencv"]):
        return "è®¡ç®—æœºè§†è§‰"
    elif any(k in text for k in ["data science", "pandas", "visualization", "analytics"]):
        return "æ•°æ®ç§‘å­¦"
    elif any(k in text for k in ["machine learning", "deep learning", "neural network"]):
        return "æœºå™¨å­¦ä¹ "
    else:
        return "å…¶ä»–AIæŠ€æœ¯"

def extract_tags(name, description):
    """æå–æŠ€æœ¯æ ‡ç­¾"""
    text = f"{name} {description}".lower()
    tags = []
    
    tag_mapping = {
        "LLM": ["llm", "language model"],
        "PyTorch": ["pytorch"],
        "TensorFlow": ["tensorflow"],
        "Transformer": ["transformer"],
        "API": ["api"],
        "Python": ["python"],
        "Chat": ["chat", "conversation"],
        "Research": ["research", "paper"],
        "Computer Vision": ["computer vision", "cv"],
        "Data Science": ["data science", "analytics"]
    }
    
    for tag, keywords in tag_mapping.items():
        if any(keyword in text for keyword in keywords):
            tags.append(tag)
    
    return ", ".join(tags[:5])  # æœ€å¤š5ä¸ªæ ‡ç­¾

def save_to_database(repos_data):
    """ä¿å­˜æ•°æ®åˆ°Cloudflare D1"""
    if not repos_data:
        print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
        return
    
    print(f"ğŸ’¾ å¼€å§‹ä¿å­˜ {len(repos_data)} æ¡æ•°æ®åˆ°æ•°æ®åº“...")
    
    sql = """
    INSERT INTO repos (id, name, owner, stars, forks, description, url, created_at, updated_at, category, tags, summary, relevance_score)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ON CONFLICT(id) DO UPDATE SET
        stars=excluded.stars,
        updated_at=excluded.updated_at,
        category=excluded.category,
        tags=excluded.tags,
        summary=excluded.summary,
        relevance_score=excluded.relevance_score,
        sync_time=CURRENT_TIMESTAMP;
    """
    
    success_count = 0
    
    # æ‰¹é‡å¤„ç†
    for repo in repos_data:
        try:
            params = [
                repo["id"], repo["name"], repo["owner"],
                repo["stars"], repo["forks"], repo["description"],
                repo["url"], repo["created_at"], repo["updated_at"],
                repo["category"], repo["tags"], repo["summary"],
                repo["relevance_score"]
            ]
            
            response = cloudflare_client.d1.database.query(
                database_id=D1_DATABASE_ID,
                account_id=CLOUDFLARE_ACCOUNT_ID,
                sql=sql,
                params=params
            )
            
            success_count += 1
            
            if success_count % 10 == 0:
                print(f"ğŸ“Š å·²ä¿å­˜ {success_count}/{len(repos_data)} æ¡æ•°æ®")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            continue
    
    print(f"ğŸ‰ æˆåŠŸä¿å­˜ {success_count} æ¡è®°å½•åˆ°æ•°æ®åº“ï¼")
    return success_count

def main_production_collection():
    """ç”Ÿäº§çº§ä¸»æ•°æ®æ”¶é›†å‡½æ•°"""
    start_time = datetime.now()
    print("ğŸš€ ç”Ÿäº§çº§GitHub AIä»“åº“æ”¶é›†å¼€å§‹")
    print(f"â° å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    search_queries = get_production_search_queries()
    time_windows = get_production_time_windows()
    
    print(f"ğŸ“Š æœç´¢å…³é”®è¯: {len(search_queries)} ä¸ª")
    print(f"â° æ—¶é—´çª—å£: {len(time_windows)} ä¸ª")
    print(f"ğŸ”„ è®¡åˆ’æœç´¢æ¬¡æ•°: {len(search_queries) * len(time_windows)} æ¬¡")
    
    all_repos = []
    unique_repos = {}
    search_count = 0
    
    # æ‰§è¡Œæœç´¢
    for keyword in search_queries:
        for time_window in time_windows:
            
            repos = execute_github_search(keyword, time_window)
            
            # å»é‡å¤„ç†
            for repo in repos:
                repo_id = str(repo.get("id"))
                if repo_id not in unique_repos:
                    unique_repos[repo_id] = repo
            
            search_count += 1
            
            # è¿›åº¦æ˜¾ç¤º
            if search_count % 10 == 0:
                print(f"ğŸ“ˆ å·²å®Œæˆ {search_count} æ¬¡æœç´¢ï¼Œæ”¶é›†åˆ° {len(unique_repos)} ä¸ªå”¯ä¸€é¡¹ç›®")
            
            # APIé™åˆ¶æ§åˆ¶
            time.sleep(1.0)  # æ¯æ¬¡æœç´¢é—´éš”1ç§’
            
            # è¾¾åˆ°è¶³å¤Ÿæ•°æ®é‡æ—¶å¯ä»¥æ—©åœ
            if len(unique_repos) >= 1000:
                print(f"âœ… å·²æ”¶é›†è¶³å¤Ÿæ•°æ® ({len(unique_repos)}ä¸ª)ï¼Œåœæ­¢æœç´¢")
                break
        
        if len(unique_repos) >= 1000:
            break
    
    all_repos = list(unique_repos.values())
    
    print(f"\nğŸ‰ æœç´¢å®Œæˆï¼")
    print(f"ğŸ“Š æ€»æœç´¢æ¬¡æ•°: {search_count}")
    print(f"ğŸ“¦ å€™é€‰é¡¹ç›®æ€»æ•°: {len(all_repos)}")
    
    # å¤„ç†å’Œè¿‡æ»¤æ•°æ®
    print(f"\nğŸ” å¼€å§‹å¤„ç†å’Œè¿‡æ»¤æ•°æ®...")
    filtered_repos = filter_and_process_repos(all_repos)
    
    print(f"âœ… è¿‡æ»¤åæœ‰æ•ˆé¡¹ç›®: {len(filtered_repos)}")
    
    if filtered_repos:
        # æŒ‰è¯„åˆ†æ’åº
        filtered_repos.sort(key=lambda x: (x["relevance_score"], x["stars"]), reverse=True)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        saved_count = save_to_database(filtered_repos)
        
        # ç»Ÿè®¡ç»“æœ
        end_time = datetime.now()
        duration = end_time - start_time
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ç”Ÿäº§çº§æ•°æ®æ”¶é›†å®Œæˆï¼")
        print(f"â° æ€»è€—æ—¶: {duration}")
        print(f"ğŸ“Š å€™é€‰é¡¹ç›®: {len(all_repos)} ä¸ª")
        print(f"âœ… æœ‰æ•ˆé¡¹ç›®: {len(filtered_repos)} ä¸ª") 
        print(f"ğŸ’¾ ä¿å­˜æˆåŠŸ: {saved_count} ä¸ª")
        print(f"ğŸ“ˆ æœ‰æ•ˆç‡: {(len(filtered_repos)/len(all_repos)*100):.1f}%")
        
        # åˆ†ç±»ç»Ÿè®¡
        categories = {}
        scores = {"é«˜åˆ†(8-10)": 0, "ä¸­åˆ†(6-7)": 0, "ä½åˆ†(2-5)": 0}
        
        for repo in filtered_repos:
            # åˆ†ç±»ç»Ÿè®¡
            cat = repo["category"]
            categories[cat] = categories.get(cat, 0) + 1
            
            # è¯„åˆ†ç»Ÿè®¡
            score = repo["relevance_score"]
            if score >= 8:
                scores["é«˜åˆ†(8-10)"] += 1
            elif score >= 6:
                scores["ä¸­åˆ†(6-7)"] += 1
            else:
                scores["ä½åˆ†(2-5)"] += 1
        
        print(f"\nğŸ“‹ åˆ†ç±»ç»Ÿè®¡:")
        for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            print(f"   {cat}: {count} ä¸ª")
        
        print(f"\nğŸ¯ è´¨é‡åˆ†å¸ƒ:")
        for level, count in scores.items():
            print(f"   {level}: {count} ä¸ª")
        
        # æˆåŠŸåˆ¤æ–­
        if len(filtered_repos) >= 200:
            print(f"\nğŸŠ æˆåŠŸè¾¾æˆç›®æ ‡ï¼æ”¶é›†åˆ° {len(filtered_repos)} æ¡æœ‰æ•ˆæ•°æ®ï¼")
        else:
            print(f"\nâš ï¸ æœªå®Œå…¨è¾¾æˆç›®æ ‡ï¼Œæ”¶é›†åˆ° {len(filtered_repos)} æ¡æœ‰æ•ˆæ•°æ®")
            print("   å»ºè®®ï¼šå¯ä»¥é™ä½è¿‡æ»¤é˜ˆå€¼æˆ–å¢åŠ æœç´¢å…³é”®è¯")
    
    else:
        print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„é¡¹ç›®æ•°æ®")

if __name__ == "__main__":
    main_production_collection()
