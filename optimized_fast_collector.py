#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆé«˜é¢‘é‡‡é›†å™¨ - è§£å†³3å¤§æ ¸å¿ƒé—®é¢˜
========================================

ä¿®å¤å†…å®¹:
1. âœ… ä¿®å¤æ—¥æœŸè§£æé”™è¯¯ (deduplication_manager.pyå·²ä¿®å¤)
2. âœ… ç®€åŒ–watchers_countå¤„ç† (æš‚æ—¶ä½¿ç”¨æœç´¢APIæ•°æ®)
3. âœ… ä¼˜åŒ–é‡‡é›†é€Ÿåº¦ (å‡å°‘é¢å¤–APIè°ƒç”¨)

Author: Claude
Date: 2025-09-12
"""

import asyncio
import logging
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any
import aiohttp
from tqdm import tqdm

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config_v2 import Config
from enhanced_keywords_config import SEARCH_ROUNDS_CONFIG
from enhanced_data_processor_v2 import EnhancedDataProcessorV2
from deduplication_manager import DeduplicationManager
from monitoring_system import CollectionMetrics, MonitoringSystem
from email_notifier import EmailNotifier

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class OptimizedHighFrequencyCollector:
    """ä¼˜åŒ–ç‰ˆé«˜é¢‘é‡‡é›†å™¨"""
    
    def __init__(self):
        self.config = Config()
        self.data_processor = EnhancedDataProcessorV2()
        self.dedup_manager = DeduplicationManager(
            cloudflare_client=None,  # å°†åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®
            config=self.config
        )
        self.monitoring = MonitoringSystem()
        self.email_notifier = EmailNotifier()
        self.session = None
        self.logger = logging.getLogger('optimized_collector')
        
        # æ€§èƒ½é…ç½®
        self.BATCH_SIZE = 50  # æ‰¹é‡å¤„ç†å¤§å°
        self.MAX_CONCURRENT = 5  # æœ€å¤§å¹¶å‘æ•°
        
    async def initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.logger.info("ğŸš€ åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆé‡‡é›†ç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–HTTPä¼šè¯
        self.session = aiohttp.ClientSession(
            headers={
                "Authorization": f"token {self.config.GITHUB_TOKEN}",
                "Accept": "application/vnd.github.v3+json"
            },
            timeout=aiohttp.ClientTimeout(total=30)
        )
        
        # åˆå§‹åŒ–å»é‡ç®¡ç†å™¨çš„Cloudflareå®¢æˆ·ç«¯
        from cloudflare import Cloudflare
        cloudflare_client = Cloudflare(api_token=self.config.CLOUDFLARE_API_TOKEN)
        self.dedup_manager.cloudflare_client = cloudflare_client
        
        self.logger.info("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
    async def search_repositories(self) -> List[Dict[str, Any]]:
        """æœç´¢ä»“åº“ - ä¼˜åŒ–ç‰ˆ"""
        all_repos = []
        
        self.logger.info("ğŸ” å¼€å§‹æ‰§è¡Œå¤šè½®æœç´¢ç­–ç•¥")
        
        for config in SEARCH_ROUNDS_CONFIG:
            round_name = config["name"]
            self.logger.info(f"ğŸš€ æ‰§è¡Œ {round_name}")
            
            keywords = config["keywords"][:3]  # é™åˆ¶å…³é”®è¯æ•°é‡ä»¥æå‡é€Ÿåº¦
            target_count = min(config.get("expected_results", 300), 300)  # é™åˆ¶å•è½®æ•°é‡
            
            round_repos = await self._search_round(keywords, target_count)
            all_repos.extend(round_repos)
            
            self.logger.info(f"âœ… {round_name} å®Œæˆ: {len(round_repos)}ä¸ªä»“åº“")
            
            # é¿å…APIé™åˆ¶
            await asyncio.sleep(1)
        
        # å»é‡
        unique_repos = {}
        for repo in all_repos:
            repo_id = repo.get('id')
            if repo_id and repo_id not in unique_repos:
                unique_repos[repo_id] = repo
        
        final_repos = list(unique_repos.values())
        self.logger.info(f"ğŸ æœç´¢å®Œæˆ: å…± {len(final_repos)} ä¸ªå»é‡åçš„ä»“åº“")
        
        # å¦‚æœæœç´¢ç»“æœå¤ªå°‘ï¼Œä½¿ç”¨å¤‡ç”¨æœç´¢ç­–ç•¥
        if len(final_repos) < 50:
            self.logger.warning(f"âš ï¸ æœç´¢ç»“æœå¤ªå°‘({len(final_repos)}ä¸ª)ï¼Œå¯ç”¨å¤‡ç”¨æœç´¢ç­–ç•¥")
            backup_repos = await self._backup_search_strategy()
            final_repos.extend(backup_repos)
            self.logger.info(f"ğŸ”„ å¤‡ç”¨æœç´¢è·å¾— {len(backup_repos)} ä¸ªä»“åº“ï¼Œæ€»è®¡ {len(final_repos)} ä¸ª")
        
        return final_repos
    
    async def _search_round(self, keywords: List[str], target_count: int) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå•è½®æœç´¢ - ä¼˜åŒ–ä¸ºå‘ç°æ–°ä»“åº“"""
        repos = []
        per_keyword = max(1, target_count // len(keywords))
        
        for keyword in keywords:
            try:
                # åŠ¨æ€æ—¶é—´è¿‡æ»¤å™¨ - åˆ†å±‚æœç´¢ç­–ç•¥
                from datetime import datetime, timedelta
                # ç¬¬ä¸€å±‚ï¼šæœç´¢æœ€è¿‘30å¤©æœ‰æ›´æ–°çš„ä»“åº“
                recent_updated_filter = "updated:>" + (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
                # ç¬¬äºŒå±‚ï¼šæœç´¢æœ€è¿‘90å¤©æœ‰æ›´æ–°çš„ä»“åº“
                extended_updated_filter = "updated:>" + (datetime.now() - timedelta(days=90)).strftime('%Y-%m-%d')
                # ç¬¬ä¸‰å±‚ï¼šæœç´¢æœ€è¿‘1å¹´æœ‰æ›´æ–°çš„ä»“åº“
                fallback_updated_filter = "updated:>" + (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')
                
                # ä¼˜å…ˆæœç´¢æœ€è¿‘30å¤©æœ‰æ›´æ–°çš„ä»“åº“ï¼Œé™ä½æ˜Ÿæ ‡è¦æ±‚
                min_stars = max(10, self.config.MIN_STARS // 2)  # é™ä½æ˜Ÿæ ‡è¦æ±‚
                query = f"{keyword} {recent_updated_filter} stars:>={min_stars}"
                
                params = {
                    "q": query,
                    "sort": "updated",  # æŒ‰æ›´æ–°æ—¶é—´æ’åºï¼Œä¼˜å…ˆæœ€è¿‘æ›´æ–°çš„ä»“åº“
                    "order": "desc",
                    "per_page": min(100, per_keyword)
                }
                
                from config_v2 import APIConfig
                async with self.session.get(
                    APIConfig.GITHUB_SEARCH_ENDPOINT,
                    params=params
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        items = data.get("items", [])
                        repos.extend(items)
                        
                        # åˆ†å±‚æœç´¢ç­–ç•¥ï¼šå¦‚æœç»“æœä¸è¶³ï¼Œé€æ­¥æ”¾å®½æ¡ä»¶
                        if len(items) < per_keyword * 0.3:  # å¦‚æœç»“æœå°‘äºé¢„æœŸçš„30%
                            self.logger.info(f"ğŸ” {keyword} æœ€è¿‘30å¤©ç»“æœä¸è¶³({len(items)}ä¸ª)ï¼Œæœç´¢æœ€è¿‘90å¤©")
                            extended_query = f"{keyword} {extended_updated_filter} stars:>={min_stars}"
                            extended_params = {
                                "q": extended_query,
                                "sort": "updated",
                                "order": "desc", 
                                "per_page": min(100, per_keyword - len(items))
                            }
                            
                            async with self.session.get(
                                APIConfig.GITHUB_SEARCH_ENDPOINT,
                                params=extended_params
                            ) as extended_response:
                                if extended_response.status == 200:
                                    extended_data = await extended_response.json()
                                    extended_items = extended_data.get("items", [])
                                    repos.extend(extended_items)
                                    self.logger.info(f"âœ… {keyword} 90å¤©æœç´¢è·å¾— {len(extended_items)} ä¸ªä»“åº“")
                                    
                                    # å¦‚æœ90å¤©ç»“æœä»ç„¶ä¸è¶³ï¼Œæœç´¢æœ€è¿‘1å¹´
                                    if len(items) + len(extended_items) < per_keyword * 0.5:
                                        self.logger.info(f"ğŸ” {keyword} 90å¤©ç»“æœä»ä¸è¶³ï¼Œæœç´¢æœ€è¿‘1å¹´")
                                        fallback_query = f"{keyword} {fallback_updated_filter} stars:>={min_stars}"
                                        fallback_params = {
                                            "q": fallback_query,
                                            "sort": "updated",
                                            "order": "desc",
                                            "per_page": min(100, per_keyword - len(items) - len(extended_items))
                                        }
                                        
                                        async with self.session.get(
                                            APIConfig.GITHUB_SEARCH_ENDPOINT,
                                            params=fallback_params
                                        ) as fallback_response:
                                            if fallback_response.status == 200:
                                                fallback_data = await fallback_response.json()
                                                fallback_items = fallback_data.get("items", [])
                                                repos.extend(fallback_items)
                                                self.logger.info(f"âœ… {keyword} 1å¹´æœç´¢è·å¾— {len(fallback_items)} ä¸ªä»“åº“")
                                    
                    elif response.status == 403:
                        self.logger.warning(f"âš ï¸ APIé™é¢‘: {keyword}")
                        await asyncio.sleep(10)
                    else:
                        self.logger.error(f"âŒ APIé”™è¯¯ {keyword}: {response.status}")
                        
            except Exception as e:
                self.logger.error(f"âŒ æœç´¢å¤±è´¥ {keyword}: {e}")
                
        return repos
    
    async def _backup_search_strategy(self) -> List[Dict[str, Any]]:
        """å¤‡ç”¨æœç´¢ç­–ç•¥ - ä½¿ç”¨æ›´å®½æ¾çš„æ¡ä»¶"""
        self.logger.info("ğŸ”„ æ‰§è¡Œå¤‡ç”¨æœç´¢ç­–ç•¥")
        backup_repos = []
        
        # ä½¿ç”¨æ›´å®½æ¾çš„æœç´¢æ¡ä»¶
        backup_keywords = [
            "machine learning", "deep learning", "neural network", 
            "artificial intelligence", "AI", "ML", "pytorch", "tensorflow"
        ]
        
        for keyword in backup_keywords[:3]:  # åªä½¿ç”¨å‰3ä¸ªå…³é”®è¯
            try:
                # ä½¿ç”¨æ›´å®½æ¾çš„æ—¶é—´èŒƒå›´å’Œæ˜Ÿæ ‡è¦æ±‚
                query = f"{keyword} stars:>=10 created:>2020-01-01"
                
                params = {
                    "q": query,
                    "sort": "stars",
                    "order": "desc",
                    "per_page": 50
                }
                
                from config_v2 import APIConfig
                async with self.session.get(
                    APIConfig.GITHUB_SEARCH_ENDPOINT,
                    params=params
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        items = data.get("items", [])
                        backup_repos.extend(items)
                        self.logger.info(f"âœ… å¤‡ç”¨æœç´¢ {keyword}: {len(items)} ä¸ªä»“åº“")
                    elif response.status == 403:
                        self.logger.warning(f"âš ï¸ å¤‡ç”¨æœç´¢APIé™é¢‘: {keyword}")
                        await asyncio.sleep(5)
                    else:
                        self.logger.error(f"âŒ å¤‡ç”¨æœç´¢APIé”™è¯¯ {keyword}: {response.status}")
                        
            except Exception as e:
                self.logger.error(f"âŒ å¤‡ç”¨æœç´¢å¤±è´¥ {keyword}: {e}")
        
        # å»é‡
        unique_backup = {}
        for repo in backup_repos:
            repo_id = repo.get('id')
            if repo_id and repo_id not in unique_backup:
                unique_backup[repo_id] = repo
        
        return list(unique_backup.values())
    
    async def process_repositories(self, repos: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†ä»“åº“æ•°æ® - ä½¿ç”¨å¢å¼ºç‰ˆå¤„ç†å™¨ç¡®ä¿watchers_countæ­£ç¡®"""
        self.logger.info(f"ğŸ”„ å¼€å§‹å¤„ç† {len(repos)} ä¸ªä»“åº“æ•°æ® (å¢å¼ºç‰ˆ)")
        
        # ä½¿ç”¨å¢å¼ºç‰ˆæ•°æ®å¤„ç†å™¨æ‰¹é‡å¤„ç†ï¼Œç¡®ä¿watchers_countæ­£ç¡®
        processed_repos = await self.data_processor.process_repositories_batch(repos, max_concurrent=10)
        
        self.logger.info(f"âœ… æ•°æ®å¤„ç†å®Œæˆ: {len(processed_repos)} ä¸ªæœ‰æ•ˆä»“åº“")
        return processed_repos
    
    async def store_repositories(self, repos: List[Dict[str, Any]]) -> Dict[str, int]:
        """å­˜å‚¨ä»“åº“æ•°æ® - æ‰¹é‡ä¼˜åŒ–"""
        self.logger.info(f"ğŸ’¾ å¼€å§‹å­˜å‚¨ {len(repos)} ä¸ªä»“åº“åˆ°æ•°æ®åº“")
        
        stats = {"new": 0, "updated": 0, "skipped": 0, "total_processed": 0}
        
        # æ‰¹é‡å¤„ç†ä»¥æå‡æ€§èƒ½
        with tqdm(repos, desc="å­˜å‚¨ä»“åº“æ•°æ®", disable=False, mininterval=2.0) as pbar:
            for repo in pbar:
                try:
                    stats["total_processed"] += 1
                    
                    # æ£€æŸ¥å»é‡é€»è¾‘
                    should_store, reason = await self.dedup_manager.should_store_repository(repo)
                    
                    if should_store:
                        # å­˜å‚¨åˆ°æ•°æ®åº“
                        success = await self.store_single_repository(repo)
                        
                        if success:
                            if "æ–°é¡¹ç›®" in reason:
                                stats["new"] += 1
                                self.logger.info(f"âœ… æ–°å¢ä»“åº“: {repo.full_name}")
                            elif "é‡è¦æ›´æ–°" in reason:
                                stats["updated"] += 1
                                self.logger.info(f"ğŸ”„ æ›´æ–°ä»“åº“: {repo.full_name}")
                            else:
                                stats["updated"] += 1
                        else:
                            stats["skipped"] += 1
                    else:
                        stats["skipped"] += 1
                        self.logger.debug(f"è·³è¿‡å­˜å‚¨: {repo.full_name} | {reason}")
                        
                except Exception as e:
                    self.logger.error(f"å­˜å‚¨å¤±è´¥ {repo.full_name}: {e}")
                    stats["skipped"] += 1
                    
                pbar.update(1)
        
        self.logger.info(f"âœ… å­˜å‚¨å®Œæˆ: æ–°å¢{stats['new']}, æ›´æ–°{stats['updated']}, è·³è¿‡{stats['skipped']}")
        return stats
    
    async def store_single_repository(self, repo) -> bool:
        """å­˜å‚¨å•ä¸ªä»“åº“åˆ°æ•°æ®åº“"""
        try:
            # å‡†å¤‡æ•°æ®
            from datetime import datetime, timezone, timedelta
            beijing_tz = timezone(timedelta(hours=8))
            current_time = datetime.now(beijing_tz).strftime('%Y-%m-%d %H:%M:%S')
            
            params = [
                repo.id, repo.full_name, repo.name, repo.owner,
                repo.description or '', repo.url, repo.stargazers_count,
                repo.forks_count, repo.watchers_count, repo.created_at,
                repo.updated_at, repo.pushed_at, repo.language or '',
                ','.join(repo.topics) if repo.topics else '',  # åºåˆ—åŒ–topics
                repo.ai_category or '', 
                ','.join(repo.ai_tags) if repo.ai_tags else '',  # åºåˆ—åŒ–ai_tags
                repo.quality_score, repo.trending_score, 1,  # collection_round
                repo.last_fork_count, repo.fork_growth, repo.collection_hash or '',
                current_time  # collection_time
            ]
            
            # æ‰§è¡Œæ’å…¥/æ›´æ–°
            response = self.dedup_manager.cloudflare_client.d1.database.query(
                database_id=self.config.D1_DATABASE_ID,
                account_id=self.config.CLOUDFLARE_ACCOUNT_ID,
                sql=self.dedup_manager.db_config.UPSERT_SQL,
                params=params
            )
            
            if response.success:
                return True
            else:
                self.logger.error(f"æ•°æ®åº“å­˜å‚¨å¤±è´¥: {response.errors}")
                return False
                
        except Exception as e:
            self.logger.error(f"å­˜å‚¨å¼‚å¸¸: {e}")
            return False
    
    async def run_optimized_collection(self):
        """è¿è¡Œä¼˜åŒ–ç‰ˆé‡‡é›†"""
        start_time = datetime.now()
        
        try:
            await self.initialize_system()
            
            # 1. æœç´¢ä»“åº“
            repos = await self.search_repositories()
            
            # 2. å¤„ç†æ•°æ®
            processed_repos = await self.process_repositories(repos)
            
            # 3. å­˜å‚¨æ•°æ®
            stats = await self.store_repositories(processed_repos)
            
            # 4. ç”ŸæˆæŠ¥å‘Š
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds() / 60
            
            self.logger.info("\n" + "="*50)
            self.logger.info("âœ… æœ€è¿‘æ›´æ–°ä»“åº“é‡‡é›†å®Œæˆ!")
            self.logger.info("="*50)
            self.logger.info(f"ğŸ“Š æ€»æœç´¢æ•°é‡: {len(processed_repos)}")
            self.logger.info(f"ğŸ†• æ–°å¢ä»“åº“: {stats['new']}")
            self.logger.info(f"ğŸ”„ æ›´æ–°ä»“åº“: {stats['updated']}")
            self.logger.info(f"â­ï¸ è·³è¿‡ä»“åº“: {stats['skipped']}")
            # å®‰å…¨è®¡ç®—æ¯”ç‡ï¼Œé¿å…é™¤é›¶é”™è¯¯
            total_processed = stats['total_processed']
            if total_processed > 0:
                new_rate = stats['new']/total_processed*100
                update_rate = stats['updated']/total_processed*100
                self.logger.info(f"ğŸ“ˆ æ–°å¢ç‡: {new_rate:.1f}%")
                self.logger.info(f"ğŸ“ˆ æ›´æ–°ç‡: {update_rate:.1f}%")
            else:
                self.logger.info(f"ğŸ“ˆ æ–°å¢ç‡: 0.0% (æ— å¤„ç†æ•°æ®)")
                self.logger.info(f"ğŸ“ˆ æ›´æ–°ç‡: 0.0% (æ— å¤„ç†æ•°æ®)")
            self.logger.info(f"â±ï¸ é‡‡é›†è€—æ—¶: {duration:.1f}åˆ†é’Ÿ")
            self.logger.info(f"ğŸš€ å¹³å‡é€Ÿåº¦: {len(processed_repos)/duration:.1f}é¡¹/åˆ†é’Ÿ")
            
            # å‘é€æˆåŠŸé€šçŸ¥é‚®ä»¶
            email_stats = {
                'total': len(processed_repos),
                'new': stats['new'],
                'updated': stats['updated'],
                'skipped': stats['skipped'],
                'duration': f"{duration:.1f}åˆ†é’Ÿ",
                'speed': f"{len(processed_repos)/duration:.1f}é¡¹/åˆ†é’Ÿ"
            }
            self.email_notifier.send_success_notification(email_stats)
            
        except Exception as e:
            self.logger.error(f"âŒ é‡‡é›†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # å‘é€å¤±è´¥é€šçŸ¥é‚®ä»¶
            try:
                self.email_notifier.send_failure_notification(str(e))
            except Exception as email_error:
                self.logger.error(f"âŒ å‘é€å¤±è´¥é€šçŸ¥é‚®ä»¶å¤±è´¥: {email_error}")
            raise
        finally:
            if self.session:
                try:
                    await self.session.close()
                except Exception as close_error:
                    self.logger.error(f"âŒ å…³é—­HTTPä¼šè¯å¤±è´¥: {close_error}")

async def main():
    """ä¸»å‡½æ•°"""
    collector = OptimizedHighFrequencyCollector()
    await collector.run_optimized_collection()

if __name__ == "__main__":
    asyncio.run(main())
