#!/usr/bin/env python3
"""
åŸºäºæ—¶é—´å»é‡çš„GitHub AIé¡¹ç›®æ”¶é›†è„šæœ¬
30å¤©å†…ä¸é‡å¤ï¼Œ30å¤©åæ»¡è¶³æ¡ä»¶å¯é‡æ–°æ”¶å½•
"""

import os
import requests
import json
import time
from datetime import datetime, timedelta
from cloudflare import Cloudflare
from dotenv import load_dotenv
from time_based_dedup_config import (
    TIME_DEDUP_CONFIG, get_time_dedup_sql, should_reentry_repo,
    get_time_dedup_stats_sql
)
from github_metrics_config import (
    build_enhanced_search_queries, calculate_comprehensive_score
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# === é…ç½® ===
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
CLOUDFLARE_API_TOKEN = os.environ.get("CLOUDFLARE_API_TOKEN")
CLOUDFLARE_ACCOUNT_ID = os.environ.get("CLOUDFLARE_ACCOUNT_ID")
D1_DATABASE_ID = os.environ.get("D1_DATABASE_ID")

if not all([GITHUB_TOKEN, CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID, D1_DATABASE_ID]):
    raise ValueError("ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚è¯·ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç¯å¢ƒå˜é‡éƒ½å·²æ­£ç¡®é…ç½®ã€‚")

# åˆå§‹åŒ–å®¢æˆ·ç«¯
cloudflare_client = Cloudflare(api_token=CLOUDFLARE_API_TOKEN)

github_headers = {
    "Authorization": f"token {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

# ================================
# ğŸ“… æ—¶é—´å»é‡æ ¸å¿ƒå‡½æ•°
# ================================

def check_existing_record(repo_id):
    """æ£€æŸ¥30å¤©å†…æ˜¯å¦å·²å­˜åœ¨è®°å½•"""
    
    sql_queries = get_time_dedup_sql()
    
    try:
        response = cloudflare_client.d1.database.query(
            database_id=D1_DATABASE_ID,
            account_id=CLOUDFLARE_ACCOUNT_ID,
            sql=sql_queries["check_existing"],
            params=[repo_id]
        )
        
        if response.success and hasattr(response, 'result') and response.result:
            results = response.result[0].results
            if results:
                return results[0]
        
        return None
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ç°æœ‰è®°å½•å¤±è´¥: {e}")
        return None

def process_repo_with_time_dedup(repo):
    """åŸºäºæ—¶é—´å»é‡å¤„ç†å•ä¸ªé¡¹ç›®"""
    
    repo_id = str(repo.get("id"))
    
    # æ£€æŸ¥30å¤©å†…æ˜¯å¦å·²å­˜åœ¨
    existing_record = check_existing_record(repo_id)
    
    # åˆ¤æ–­å¤„ç†ç­–ç•¥
    should_process, reason, action = should_reentry_repo(existing_record, repo)
    
    result = {
        "repo_id": repo_id,
        "name": repo.get("name"),
        "action": action,
        "reason": reason,
        "should_process": should_process
    }
    
    if not should_process:
        return result
    
    # è®¡ç®—é¡¹ç›®æ•°æ®
    enhanced_data = enhance_repo_data(repo)
    
    if action == "insert" or action == "reinsert":
        success = insert_new_record(enhanced_data)
        result["success"] = success
        result["operation"] = "æ’å…¥æ–°è®°å½•"
        
    elif action == "update":
        success = update_existing_record(repo_id, enhanced_data)
        result["success"] = success
        result["operation"] = "æ›´æ–°ç°æœ‰è®°å½•"
    
    return result

def enhance_repo_data(repo):
    """å¢å¼ºé¡¹ç›®æ•°æ®"""
    
    # è®¡ç®—è¯„åˆ†
    score = calculate_comprehensive_score(repo)
    
    # åˆ†ç±»
    category = classify_project(repo)
    
    # æ ‡ç­¾
    tags = extract_tags(repo)
    
    # æ‘˜è¦
    summary = generate_summary(repo)
    
    return {
        "id": str(repo.get("id")),
        "name": repo.get("name"),
        "owner": repo.get("owner", {}).get("login"),
        "stars": repo.get("stargazers_count", 0),
        "forks": repo.get("forks_count", 0),
        "description": repo.get("description", ""),
        "url": repo.get("html_url"),
        "created_at": repo.get("created_at"),
        "updated_at": repo.get("pushed_at"),
        "category": category,
        "tags": tags,
        "summary": summary,
        "relevance_score": score
    }

def classify_project(repo):
    """é¡¹ç›®åˆ†ç±»"""
    name = repo.get("name", "").lower()
    description = repo.get("description", "").lower() if repo.get("description") else ""
    text = f"{name} {description}"
    
    if any(k in text for k in ["llm", "large language", "gpt", "language model"]):
        if any(k in text for k in ["api", "server", "serving"]):
            return "LLMæœåŠ¡ä¸å·¥å…·"
        elif any(k in text for k in ["chat", "assistant", "bot"]):
            return "LLMåº”ç”¨"
        else:
            return "LLMç ”ç©¶"
    elif any(k in text for k in ["rag", "retrieval", "vector"]):
        return "RAGæŠ€æœ¯"
    elif any(k in text for k in ["diffusion", "stable-diffusion", "generation"]):
        return "ç”Ÿæˆå¼AI"
    elif any(k in text for k in ["computer vision", "object detection", "yolo"]):
        return "è®¡ç®—æœºè§†è§‰"
    elif any(k in text for k in ["data science", "analytics", "visualization"]):
        return "æ•°æ®ç§‘å­¦"
    else:
        return "é€šç”¨AI"

def extract_tags(repo):
    """æå–æŠ€æœ¯æ ‡ç­¾"""
    text = f"{repo.get('name', '')} {repo.get('description', '')}".lower()
    tags = []
    
    tag_keywords = {
        "LLM": ["llm", "language model"],
        "PyTorch": ["pytorch"],
        "TensorFlow": ["tensorflow"],
        "API": ["api"],
        "Research": ["research", "paper"]
    }
    
    for tag, keywords in tag_keywords.items():
        if any(keyword in text for keyword in keywords):
            tags.append(tag)
    
    return ", ".join(tags[:5])

def generate_summary(repo):
    """ç”Ÿæˆé¡¹ç›®æ‘˜è¦"""
    name = repo.get("name", "")
    description = repo.get("description", "")
    stars = repo.get("stargazers_count", 0)
    
    if description:
        desc_snippet = description.split('.')[0][:50]
        return f"{name} - {desc_snippet} (â­{stars})"
    else:
        return f"{name} - AIé¡¹ç›® (â­{stars})"

def insert_new_record(enhanced_data):
    """æ’å…¥æ–°è®°å½•"""
    
    sql_queries = get_time_dedup_sql()
    
    try:
        params = [
            enhanced_data["id"], enhanced_data["name"], enhanced_data["owner"],
            enhanced_data["stars"], enhanced_data["forks"], enhanced_data["description"],
            enhanced_data["url"], enhanced_data["created_at"], enhanced_data["updated_at"],
            enhanced_data["category"], enhanced_data["tags"], enhanced_data["summary"],
            enhanced_data["relevance_score"]
        ]
        
        response = cloudflare_client.d1.database.query(
            database_id=D1_DATABASE_ID,
            account_id=CLOUDFLARE_ACCOUNT_ID,
            sql=sql_queries["insert_new"],
            params=params
        )
        
        return response.success
        
    except Exception as e:
        print(f"âŒ æ’å…¥è®°å½•å¤±è´¥: {e}")
        return False

def update_existing_record(repo_id, enhanced_data):
    """æ›´æ–°ç°æœ‰è®°å½•"""
    
    sql_queries = get_time_dedup_sql()
    
    try:
        params = [
            enhanced_data["stars"], enhanced_data["updated_at"],
            enhanced_data["category"], enhanced_data["tags"],
            enhanced_data["summary"], enhanced_data["relevance_score"],
            repo_id
        ]
        
        response = cloudflare_client.d1.database.query(
            database_id=D1_DATABASE_ID,
            account_id=CLOUDFLARE_ACCOUNT_ID,
            sql=sql_queries["update_existing"],
            params=params
        )
        
        return response.success
        
    except Exception as e:
        print(f"âŒ æ›´æ–°è®°å½•å¤±è´¥: {e}")
        return False

# ================================
# ğŸ” æœç´¢å’Œæ”¶é›†
# ================================

def execute_time_based_search():
    """æ‰§è¡ŒåŸºäºæ—¶é—´å»é‡çš„æœç´¢"""
    
    print("ğŸš€ å¼€å§‹åŸºäºæ—¶é—´å»é‡çš„æ•°æ®æ”¶é›†")
    print(f"ğŸ“… å»é‡çª—å£: {TIME_DEDUP_CONFIG['dedup_window_days']} å¤©")
    print("=" * 60)
    
    # è·å–æœç´¢ç­–ç•¥
    search_strategies, time_ranges = build_enhanced_search_queries()
    
    # AIå…³é”®è¯
    ai_keywords = [
        "LLM", "transformer", "artificial-intelligence",
        "machine-learning", "deep-learning", "computer-vision",
        "diffusion", "RAG", "pytorch", "tensorflow"
    ]
    
    all_repos = []
    search_count = 0
    
    # æ‰§è¡Œæœç´¢
    for strategy in search_strategies[:2]:  # ä½¿ç”¨å‰2ç§ç­–ç•¥
        for keyword in ai_keywords[:5]:  # ä½¿ç”¨å‰5ä¸ªå…³é”®è¯
            
            # æ„å»ºæŸ¥è¯¢
            if strategy["name"] == "star_projects":
                query = f"{keyword} stars:>1000 pushed:>=2025-06-07 is:public archived:false"
            else:
                query = f"{keyword} stars:>100 created:>=2025-06-07 is:public archived:false"
            
            try:
                print(f"ğŸ” æœç´¢: {keyword} ({strategy['name']})")
                
                params = {
                    "q": query,
                    "sort": "stars",
                    "order": "desc",
                    "per_page": 50  # å‡å°‘æ¯æ¬¡æŸ¥è¯¢æ•°é‡ï¼Œä¸“æ³¨è´¨é‡
                }
                
                response = requests.get(
                    "https://api.github.com/search/repositories",
                    headers=github_headers,
                    params=params
                )
                response.raise_for_status()
                
                data = response.json()
                repos = data.get("items", [])
                
                print(f"   ğŸ“Š æ‰¾åˆ° {len(repos)} ä¸ªé¡¹ç›®")
                all_repos.extend(repos)
                
                search_count += 1
                time.sleep(2)  # APIé™åˆ¶
                
                if len(all_repos) >= 200:  # æ§åˆ¶å€™é€‰æ•°é‡
                    break
                    
            except Exception as e:
                print(f"   âŒ æœç´¢å¤±è´¥: {e}")
                continue
        
        if len(all_repos) >= 200:
            break
    
    print(f"\nğŸ“¦ æœç´¢å®Œæˆï¼Œæ”¶é›†åˆ° {len(all_repos)} ä¸ªå€™é€‰é¡¹ç›®")
    return all_repos

def process_repos_with_time_dedup(repos):
    """ä½¿ç”¨æ—¶é—´å»é‡å¤„ç†æ‰€æœ‰é¡¹ç›®"""
    
    print(f"\nğŸ”„ å¼€å§‹å¤„ç† {len(repos)} ä¸ªé¡¹ç›® (30å¤©å»é‡)")
    print("=" * 60)
    
    results = {
        "inserted": 0,
        "updated": 0, 
        "skipped": 0,
        "reinserted": 0,
        "errors": 0
    }
    
    processed_repos = []
    
    for i, repo in enumerate(repos):
        try:
            # åŸºç¡€è¿‡æ»¤
            if repo.get("fork", False) or repo.get("archived", False):
                continue
                
            if not repo.get("description") or len(repo.get("description", "")) < 20:
                continue
            
            # æ—¶é—´å»é‡å¤„ç†
            result = process_repo_with_time_dedup(repo)
            
            if result["should_process"]:
                processed_repos.append(result)
                
                if result["action"] == "insert":
                    results["inserted"] += 1
                    print(f"âœ… æ’å…¥: {result['name']} - {result['reason']}")
                elif result["action"] == "update":
                    results["updated"] += 1
                    print(f"ğŸ”„ æ›´æ–°: {result['name']} - {result['reason']}")
                elif result["action"] == "reinsert":
                    results["reinserted"] += 1
                    print(f"ğŸ”„ é‡æ–°æ”¶å½•: {result['name']} - {result['reason']}")
            else:
                results["skipped"] += 1
                if i % 10 == 0:  # æ¯10ä¸ªæ˜¾ç¤ºä¸€æ¬¡è·³è¿‡ä¿¡æ¯
                    print(f"â­ï¸ è·³è¿‡: {result['name']} - {result['reason']}")
                    
        except Exception as e:
            results["errors"] += 1
            print(f"âŒ å¤„ç†å‡ºé”™: {e}")
            continue
    
    return results, processed_repos

# ================================
# ğŸ“Š ç»Ÿè®¡å’ŒæŠ¥å‘Š
# ================================

def show_time_dedup_stats():
    """æ˜¾ç¤ºæ—¶é—´å»é‡ç»Ÿè®¡"""
    
    print("\nğŸ“Š æ—¶é—´å»é‡ç»Ÿè®¡")
    print("=" * 40)
    
    try:
        stats_queries = get_time_dedup_stats_sql()
        
        # 30å¤©å†…è®°å½•æ•°
        response = cloudflare_client.d1.database.query(
            database_id=D1_DATABASE_ID,
            account_id=CLOUDFLARE_ACCOUNT_ID,
            sql=stats_queries["recent_records"]
        )
        
        if response.success and hasattr(response, 'result') and response.result:
            count = response.result[0].results[0]["count"]
            print(f"ğŸ“… 30å¤©å†…è®°å½•æ•°: {count}")
        
    except Exception as e:
        print(f"âŒ ç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {e}")

# ================================
# ğŸš€ ä¸»ç¨‹åº
# ================================

def main_time_based_collection():
    """åŸºäºæ—¶é—´å»é‡çš„ä¸»æ”¶é›†ç¨‹åº"""
    
    start_time = datetime.now()
    print("ğŸš€ åŸºäºæ—¶é—´å»é‡çš„GitHub AIé¡¹ç›®æ”¶é›†")
    print(f"â° å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“… å»é‡ç­–ç•¥: 30å¤©å†…ä¸é‡å¤ï¼Œæ»¡è¶³æ¡ä»¶å¯é‡æ–°æ”¶å½•")
    
    # æ˜¾ç¤ºå½“å‰ç»Ÿè®¡
    show_time_dedup_stats()
    
    # æ‰§è¡Œæœç´¢
    all_repos = execute_time_based_search()
    
    if not all_repos:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å€™é€‰é¡¹ç›®")
        return
    
    # å»é‡å¤„ç†
    unique_repos = {str(repo.get("id")): repo for repo in all_repos}.values()
    print(f"ğŸ”„ å»é‡åå€™é€‰é¡¹ç›®: {len(list(unique_repos))} ä¸ª")
    
    # æ—¶é—´å»é‡å¤„ç†
    results, processed_repos = process_repos_with_time_dedup(list(unique_repos))
    
    # ç»“æœç»Ÿè®¡
    end_time = datetime.now()
    duration = end_time - start_time
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ—¶é—´å»é‡æ”¶é›†å®Œæˆï¼")
    print(f"â° æ€»è€—æ—¶: {duration}")
    print(f"ğŸ“Š å¤„ç†ç»“æœ:")
    print(f"   âœ… æ–°æ’å…¥: {results['inserted']} ä¸ª")
    print(f"   ğŸ”„ æ›´æ–°: {results['updated']} ä¸ª")
    print(f"   ğŸ”„ é‡æ–°æ”¶å½•: {results['reinserted']} ä¸ª")
    print(f"   â­ï¸ è·³è¿‡: {results['skipped']} ä¸ª")
    print(f"   âŒ é”™è¯¯: {results['errors']} ä¸ª")
    
    total_processed = results['inserted'] + results['updated'] + results['reinserted']
    print(f"ğŸ“ˆ æœ‰æ•ˆå¤„ç†: {total_processed} ä¸ªé¡¹ç›®")
    
    if total_processed > 0:
        print(f"\nğŸ¯ æ—¶é—´å»é‡æ•ˆæœ:")
        print(f"   - é¿å…äº† {results['skipped']} ä¸ª30å¤©å†…é‡å¤")
        print(f"   - æ–°å‘ç°äº† {results['inserted']} ä¸ªé¡¹ç›®")
        print(f"   - æ›´æ–°äº† {results['updated']} ä¸ªæ´»è·ƒé¡¹ç›®")
        print(f"   - é‡æ–°æ”¶å½•äº† {results['reinserted']} ä¸ªå‘å±•é¡¹ç›®")
    
    # æ˜¾ç¤ºæœ€æ–°ç»Ÿè®¡
    show_time_dedup_stats()

if __name__ == "__main__":
    main_time_based_collection()
