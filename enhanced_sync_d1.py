#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆGitHub AIä»“åº“æ•°æ®æ”¶é›†è„šæœ¬
ç›®æ ‡ï¼šæ¯å¤©æ”¶é›†200+æ¡æœ‰æ•ˆAIé¡¹ç›®æ•°æ®
ç­–ç•¥ï¼šå¤šè½®æ¬¡ã€å¤šç»´åº¦ã€å¤šæ—¶é—´çª—å£æœç´¢
"""

import os
import requests
import json
import time
from datetime import datetime, timedelta
from cloudflare import Cloudflare
from dotenv import load_dotenv
from enhanced_search_config import (
    ENHANCED_SEARCH_CONFIG, TECH_KEYWORD_GROUPS, TRENDING_KEYWORDS,
    LANGUAGE_COMBINATIONS, SORT_STRATEGIES, ENHANCED_FILTER_CONFIG,
    HIGH_VALUE_KEYWORDS, EXECUTION_STRATEGY, generate_search_queries,
    get_time_windows
)

# ä»åŸé…ç½®å¯¼å…¥å¿…è¦çš„å‡½æ•°
from search_config import AI_RELEVANCE_THRESHOLD

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- é…ç½®éƒ¨åˆ† ---
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
CLOUDFLARE_API_TOKEN = os.environ.get("CLOUDFLARE_API_TOKEN")
CLOUDFLARE_ACCOUNT_ID = os.environ.get("CLOUDFLARE_ACCOUNT_ID")
D1_DATABASE_ID = os.environ.get("D1_DATABASE_ID")

if not all([GITHUB_TOKEN, CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID, D1_DATABASE_ID]):
    raise ValueError("ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚è¯·ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç¯å¢ƒå˜é‡éƒ½å·²æ­£ç¡®é…ç½®ã€‚")

# --- åˆå§‹åŒ–å®¢æˆ·ç«¯ ---
cloudflare_client = Cloudflare(api_token=CLOUDFLARE_API_TOKEN)

# GitHub APIè®¾ç½®
github_url = "https://api.github.com/search/repositories"
github_headers = {
    "Authorization": f"token {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

# ================================
# ğŸ” å¢å¼ºæœç´¢å‡½æ•°
# ================================

def calculate_ai_relevance_enhanced(repo):
    """å¢å¼ºç‰ˆAIç›¸å…³æ€§è¯„åˆ†ç®—æ³•"""
    name = repo.get("name", "").lower()
    description = repo.get("description", "").lower() if repo.get("description") else ""
    full_text = f"{name} {description}"
    
    score = 0
    
    # é«˜ä»·å€¼å…³é”®è¯è¯„åˆ† (æ¯ä¸ª3åˆ†)
    high_value_matches = sum(1 for keyword in HIGH_VALUE_KEYWORDS 
                           if keyword.lower() in full_text)
    score += high_value_matches * 3
    
    # æŠ€æœ¯æ ˆåŒ¹é…è¯„åˆ† (æ¯ç»„2åˆ†)
    for group_keywords in TECH_KEYWORD_GROUPS.values():
        if any(keyword.lower() in full_text for keyword in group_keywords):
            score += 2
    
    # çƒ­é—¨æŠ€æœ¯è¯„åˆ† (æ¯ä¸ª1åˆ†)
    trending_matches = sum(1 for keyword in TRENDING_KEYWORDS 
                         if keyword.lower() in full_text)
    score += trending_matches
    
    # é¡¹ç›®è´¨é‡æŒ‡æ ‡
    stars = repo.get("stargazers_count", 0)
    if stars >= 1000:
        score += 2
    elif stars >= 500:
        score += 1
        
    # æœ€è¿‘æ´»è·ƒåº¦
    updated_at = repo.get("updated_at", "")
    if updated_at:
        try:
            updated = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
            days_ago = (datetime.now().replace(tzinfo=updated.tzinfo) - updated).days
            if days_ago <= 30:
                score += 1
        except:
            pass
    
    # è´Ÿé¢å…³é”®è¯æ‰£åˆ†
    negative_keywords = ["tutorial", "example", "demo", "course", "learning", "study"]
    negative_matches = sum(1 for keyword in negative_keywords 
                         if keyword in full_text)
    score -= negative_matches
    
    return min(10, max(0, score))

def filter_ai_repos_enhanced(repos):
    """å¢å¼ºç‰ˆAIä»“åº“è¿‡æ»¤"""
    filtered = []
    
    for repo in repos:
        # åŸºç¡€è¿‡æ»¤
        if repo.get("fork", False) and ENHANCED_FILTER_CONFIG.get("exclude_forks", True):
            continue
            
        if repo.get("archived", False) and ENHANCED_FILTER_CONFIG.get("exclude_archived", True):
            continue
            
        description = repo.get("description", "")
        if len(description) < ENHANCED_FILTER_CONFIG.get("min_description_length", 20):
            continue
        
        # AIç›¸å…³æ€§è¯„åˆ†
        score = calculate_ai_relevance_enhanced(repo)
        
        if score >= ENHANCED_FILTER_CONFIG.get("ai_relevance_threshold", 1):
            repo['relevance_score'] = score
            filtered.append(repo)
    
    return filtered

# ================================
# ğŸ¯ å¤šç­–ç•¥æœç´¢æ‰§è¡Œ
# ================================

def execute_search_query(query_config, time_window, sort_config):
    """æ‰§è¡Œå•æ¬¡æœç´¢æŸ¥è¯¢"""
    
    # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
    search_terms = query_config["terms"]
    
    # ä½¿ç”¨æ—¶é—´çª—å£è‡ªå¸¦çš„æ˜Ÿæ ‡è¦æ±‚
    star_threshold = time_window.get("min_stars", 20)
    
    # æ·»åŠ åŸºç¡€è¿‡æ»¤æ¡ä»¶
    filters = [
        f"stars:>{star_threshold}",
        f"created:{time_window['range']}",
        "is:public",
        "archived:false"
    ]
    
    # å¦‚æœæ˜¯è¯­è¨€ç‰¹å®šæœç´¢ï¼Œè¯­è¨€è¿‡æ»¤å·²åœ¨termsä¸­
    if not query_config["name"].startswith("lang_"):
        filters.append("language:Python OR language:JavaScript OR language:TypeScript OR language:Rust OR language:Go")
    
    query_string = f"({search_terms}) {' '.join(filters)}"
    
    params = {
        "q": query_string,
        "sort": sort_config["sort"],
        "order": sort_config["order"],
        "per_page": ENHANCED_SEARCH_CONFIG["per_page"]
    }
    
    try:
        print(f"ğŸ” æœç´¢: {query_config['name']} | æ—¶é—´çª—å£: {time_window['name']} | æ’åº: {sort_config['sort']}")
        print(f"   æŸ¥è¯¢: {query_string[:100]}...")
        
        response = requests.get(github_url, headers=github_headers, params=params)
        response.raise_for_status()
        
        data = response.json()
        repos = data.get("items", [])
        
        print(f"   æ‰¾åˆ° {len(repos)} ä¸ªå€™é€‰é¡¹ç›®")
        
        return repos
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        return []

def enhanced_data_collection():
    """å¢å¼ºç‰ˆæ•°æ®æ”¶é›†ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆæ•°æ®æ”¶é›†...")
    print(f"ğŸ¯ ç›®æ ‡ï¼šæ”¶é›† {EXECUTION_STRATEGY['target_valid']} æ¡æœ‰æ•ˆAIé¡¹ç›®")
    
    all_repos = []
    search_count = 0
    
    # ç”Ÿæˆæœç´¢æŸ¥è¯¢
    search_queries = generate_search_queries()
    time_windows = get_time_windows()
    
    print(f"ğŸ“Š ç”Ÿæˆäº† {len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")
    print(f"â° é…ç½®äº† {len(time_windows)} ä¸ªæ—¶é—´çª—å£")
    print(f"ğŸ”„ è®¡åˆ’æ‰§è¡Œ {len(search_queries) * len(time_windows)} æ¬¡æœç´¢")
    
    # å¤šè½®æœç´¢
    for query_config in search_queries[:EXECUTION_STRATEGY["primary_searches"]]:
        for time_window in time_windows[:4]:  # ä½¿ç”¨å‰4ä¸ªæ—¶é—´çª—å£
            for sort_config in SORT_STRATEGIES:
                
                # æ‰§è¡Œæœç´¢ï¼ˆæ˜Ÿæ ‡è¦æ±‚åœ¨æ—¶é—´çª—å£ä¸­å®šä¹‰ï¼‰
                repos = execute_search_query(query_config, time_window, sort_config)
                all_repos.extend(repos)
                
                search_count += 1
                
                # è¿›åº¦æ˜¾ç¤º
                if search_count % 10 == 0:
                    print(f"ğŸ“ˆ å·²å®Œæˆ {search_count} æ¬¡æœç´¢ï¼Œæ”¶é›†åˆ° {len(all_repos)} ä¸ªå€™é€‰é¡¹ç›®")
                
                # APIé™åˆ¶æ§åˆ¶
                time.sleep(EXECUTION_STRATEGY["request_delay"])
                
                # å¦‚æœå·²ç»æ”¶é›†è¶³å¤Ÿå¤šçš„å€™é€‰é¡¹ç›®ï¼Œå¯ä»¥æå‰ç»“æŸ
                if len(all_repos) >= EXECUTION_STRATEGY["target_candidates"]:
                    print(f"âœ… å·²è¾¾åˆ°å€™é€‰é¡¹ç›®ç›®æ ‡ ({len(all_repos)}ä¸ª)ï¼Œåœæ­¢æœç´¢")
                    break
            
            if len(all_repos) >= EXECUTION_STRATEGY["target_candidates"]:
                break
        
        if len(all_repos) >= EXECUTION_STRATEGY["target_candidates"]:
            break
        
        # æ¯ç»„æŸ¥è¯¢åçŸ­æš‚ä¼‘æ¯
        time.sleep(EXECUTION_STRATEGY["batch_delay"])
    
    print(f"\nğŸ‰ æœç´¢å®Œæˆï¼")
    print(f"ğŸ“Š æ€»æœç´¢æ¬¡æ•°: {search_count}")
    print(f"ğŸ“¦ å€™é€‰é¡¹ç›®æ€»æ•°: {len(all_repos)}")
    
    return all_repos

# ================================
# ğŸ”„ æ•°æ®å¤„ç†ä¸å­˜å‚¨
# ================================

def categorize_ai_project_enhanced(name, description):
    """å¢å¼ºç‰ˆAIé¡¹ç›®åˆ†ç±»"""
    name_lower = name.lower() if name else ""
    desc_lower = description.lower() if description else ""
    full_text = f"{name_lower} {desc_lower}"
    
    # æ›´ç²¾ç¡®çš„åˆ†ç±»é€»è¾‘
    if any(keyword in full_text for keyword in ["gpt", "llm", "large language model", "language model", "chatgpt", "claude", "llama"]):
        if any(keyword in full_text for keyword in ["api", "server", "inference", "serving", "engine"]):
            return "LLMæœåŠ¡ä¸å·¥å…·"
        elif any(keyword in full_text for keyword in ["chat", "assistant", "bot", "conversation"]):
            return "LLMåº”ç”¨"
        else:
            return "LLMç ”ç©¶"
    
    elif any(keyword in full_text for keyword in ["rag", "retrieval", "vector", "embedding", "semantic search"]):
        return "RAGæŠ€æœ¯"
    
    elif any(keyword in full_text for keyword in ["diffusion", "stable-diffusion", "text-to-image", "generation", "dall-e", "midjourney"]):
        return "ç”Ÿæˆå¼AI"
    
    elif any(keyword in full_text for keyword in ["computer vision", "cv", "yolo", "object detection", "image recognition", "opencv", "segmentation"]):
        return "è®¡ç®—æœºè§†è§‰"
    
    elif any(keyword in full_text for keyword in ["data science", "pandas", "numpy", "jupyter", "analytics", "visualization"]):
        return "æ•°æ®ç§‘å­¦"
    
    elif any(keyword in full_text for keyword in ["machine learning", "ml", "deep learning", "neural network", "tensorflow", "pytorch"]):
        return "æœºå™¨å­¦ä¹ "
    
    elif any(keyword in full_text for keyword in ["agent", "autonomous", "multi-agent", "workflow", "automation"]):
        return "æ™ºèƒ½ä½“ä¸è‡ªåŠ¨åŒ–"
    
    elif any(keyword in full_text for keyword in ["speech", "audio", "voice", "whisper", "tts", "asr"]):
        return "è¯­éŸ³AI"
    
    elif any(keyword in full_text for keyword in ["robotics", "robot", "embodied", "simulation"]):
        return "æœºå™¨äººä¸ä»¿çœŸ"
    
    else:
        return "å…¶ä»–AIæŠ€æœ¯"

def process_and_save_repos_enhanced(repos):
    """å¢å¼ºç‰ˆæ•°æ®å¤„ç†å’Œä¿å­˜"""
    print(f"\nğŸ” å¼€å§‹å¤„ç† {len(repos)} ä¸ªå€™é€‰é¡¹ç›®...")
    
    # å»é‡å¤„ç†
    unique_repos = {}
    for repo in repos:
        repo_id = str(repo.get("id"))
        if repo_id not in unique_repos:
            unique_repos[repo_id] = repo
    
    repos = list(unique_repos.values())
    print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(repos)} ä¸ªé¡¹ç›®")
    
    # AIç›¸å…³æ€§è¿‡æ»¤
    filtered_repos = filter_ai_repos_enhanced(repos)
    print(f"ğŸ¯ AIè¿‡æ»¤åå‰©ä½™ {len(filtered_repos)} ä¸ªé¡¹ç›®")
    
    if not filtered_repos:
        print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„é¡¹ç›®")
        return
    
    # æ•°æ®å¤„ç†å’Œå¢å¼º
    processed_repos = []
    for repo in filtered_repos:
        try:
            # åŸºç¡€ä¿¡æ¯
            processed_repo = {
                "id": str(repo.get("id")),
                "name": repo.get("name"),
                "owner": repo.get("owner", {}).get("login"),
                "stars": repo.get("stargazers_count", 0),
                "forks": repo.get("forks_count", 0),
                "description": repo.get("description"),
                "url": repo.get("html_url"),
                "created_at": repo.get("created_at"),
                "updated_at": repo.get("pushed_at"),
                "relevance_score": repo.get("relevance_score", 0)
            }
            
            # æ™ºèƒ½åˆ†æ
            processed_repo["category"] = categorize_ai_project_enhanced(
                processed_repo["name"], 
                processed_repo["description"]
            )
            
            # ç®€åŒ–çš„æ ‡ç­¾æå–
            tags = []
            desc_lower = (processed_repo["description"] or "").lower()
            name_lower = (processed_repo["name"] or "").lower()
            full_text = f"{name_lower} {desc_lower}"
            
            # æŠ€æœ¯æ ‡ç­¾
            if "llm" in full_text or "language model" in full_text:
                tags.append("LLM")
            if "transformer" in full_text:
                tags.append("Transformer") 
            if "pytorch" in full_text:
                tags.append("PyTorch")
            if "tensorflow" in full_text:
                tags.append("TensorFlow")
            if "api" in full_text:
                tags.append("API")
            if "cli" in full_text:
                tags.append("CLI")
                
            processed_repo["tags"] = ", ".join(tags[:5])  # æœ€å¤š5ä¸ªæ ‡ç­¾
            
            # ç”Ÿæˆæ‘˜è¦
            processed_repo["summary"] = f"{processed_repo['name']} - {(processed_repo['description'] or '').split('.')[0]}"[:100]
            
            processed_repos.append(processed_repo)
            
        except Exception as e:
            print(f"âŒ å¤„ç†é¡¹ç›®æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"âœ… æˆåŠŸå¤„ç† {len(processed_repos)} ä¸ªé¡¹ç›®")
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    save_to_database_enhanced(processed_repos)
    
    return processed_repos

def save_to_database_enhanced(repos_data):
    """å¢å¼ºç‰ˆæ•°æ®åº“ä¿å­˜"""
    if not repos_data:
        print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
        return
    
    print(f"ğŸ’¾ å¼€å§‹ä¿å­˜ {len(repos_data)} æ¡æ•°æ®åˆ°æ•°æ®åº“...")
    
    # å‡†å¤‡SQLè¯­å¥
    sql = """
    INSERT INTO repos (id, name, owner, stars, forks, description, url, created_at, updated_at, category, tags, summary, relevance_score)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ON CONFLICT(id) DO UPDATE SET
        stars=excluded.stars,
        updated_at=excluded.updated_at,
        category=excluded.category,
        tags=excluded.tags,
        summary=excluded.summary,
        relevance_score=excluded.relevance_score,
        sync_time=CURRENT_TIMESTAMP;
    """
    
    # æ‰¹é‡å¤„ç†
    batch_size = 10
    success_count = 0
    
    for i in range(0, len(repos_data), batch_size):
        batch = repos_data[i:i+batch_size]
        commands = []
        
        for repo in batch:
            commands.append({
                "sql": sql,
                "params": [
                    repo["id"], repo["name"], repo["owner"],
                    repo["stars"], repo["forks"], repo["description"],
                    repo["url"], repo["created_at"], repo["updated_at"],
                    repo["category"], repo["tags"], repo["summary"],
                    repo["relevance_score"]
                ]
            })
        
        try:
            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            response = cloudflare_client.d1.database.query(
                database_id=D1_DATABASE_ID,
                account_id=CLOUDFLARE_ACCOUNT_ID,
                sql=sql,
                params=commands[0]["params"] if commands else []
            )
            
            success_count += len(batch)
            print(f"ğŸ“Š å·²ä¿å­˜ {success_count}/{len(repos_data)} æ¡æ•°æ®")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ‰¹æ¬¡å¤±è´¥: {e}")
            
            # å°è¯•é€æ¡ä¿å­˜
            for cmd in commands:
                try:
                    cloudflare_client.d1.database.query(
                        database_id=D1_DATABASE_ID,
                        account_id=CLOUDFLARE_ACCOUNT_ID,
                        sql=cmd["sql"],
                        params=cmd["params"]
                    )
                    success_count += 1
                except Exception as e2:
                    print(f"âŒ ä¿å­˜å•æ¡è®°å½•å¤±è´¥: {e2}")
    
    print(f"ğŸ‰ æˆåŠŸä¿å­˜ {success_count} æ¡è®°å½•åˆ°æ•°æ®åº“ï¼")

# ================================
# ğŸš€ ä¸»ç¨‹åº
# ================================

def main_enhanced():
    """å¢å¼ºç‰ˆä¸»ç¨‹åº"""
    start_time = datetime.now()
    print(f"ğŸš€ å¢å¼ºç‰ˆGitHub AIä»“åº“æ”¶é›†å¼€å§‹")
    print(f"â° å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ ç›®æ ‡: {EXECUTION_STRATEGY['target_valid']} æ¡æœ‰æ•ˆæ•°æ®")
    print("=" * 60)
    
    try:
        # æ‰§è¡Œæ•°æ®æ”¶é›†
        all_repos = enhanced_data_collection()
        
        # å¤„ç†å’Œä¿å­˜æ•°æ®
        processed_repos = process_and_save_repos_enhanced(all_repos)
        
        # ç»Ÿè®¡ç»“æœ
        end_time = datetime.now()
        duration = end_time - start_time
        
        print("\n" + "=" * 60)
        print("ğŸ‰ å¢å¼ºç‰ˆæ•°æ®æ”¶é›†å®Œæˆï¼")
        print(f"â° æ€»è€—æ—¶: {duration}")
        print(f"ğŸ“Š å€™é€‰é¡¹ç›®: {len(all_repos)} ä¸ª")
        print(f"âœ… æœ‰æ•ˆé¡¹ç›®: {len(processed_repos) if processed_repos else 0} ä¸ª")
        print(f"ğŸ“ˆ æœ‰æ•ˆç‡: {(len(processed_repos)/len(all_repos)*100):.1f}%" if all_repos and processed_repos else "N/A")
        
        if processed_repos:
            # åˆ†ç±»ç»Ÿè®¡
            categories = {}
            for repo in processed_repos:
                cat = repo["category"]
                categories[cat] = categories.get(cat, 0) + 1
            
            print("\nğŸ“‹ åˆ†ç±»ç»Ÿè®¡:")
            for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                print(f"   {cat}: {count} ä¸ª")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main_enhanced()
