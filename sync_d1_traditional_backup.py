#!/usr/bin/env python3
"""
åŸºäºGitHubå®˜æ–¹æŒ‡æ ‡çš„æ™ºèƒ½AIé¡¹ç›®æ”¶é›†è„šæœ¬
ä½¿ç”¨å¤šç»´åº¦è¯„ä¼°ä½“ç³»ï¼Œç›®æ ‡æ¯å¤©200+æ¡é«˜è´¨é‡æ•°æ®
"""

import os
import requests
import json
import time
from datetime import datetime, timedelta
from cloudflare import Cloudflare
from dotenv import load_dotenv
from github_metrics_config import (
    CORE_METRICS_CONFIG, ACTIVITY_METRICS_CONFIG, QUALITY_METRICS_CONFIG,
    AI_SPECIFIC_METRICS, SEARCH_OPTIMIZATION_CONFIG,
    build_enhanced_search_queries, calculate_comprehensive_score
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# === é…ç½® ===
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
CLOUDFLARE_API_TOKEN = os.environ.get("CLOUDFLARE_API_TOKEN") 
CLOUDFLARE_ACCOUNT_ID = os.environ.get("CLOUDFLARE_ACCOUNT_ID")
D1_DATABASE_ID = os.environ.get("D1_DATABASE_ID")

if not all([GITHUB_TOKEN, CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID, D1_DATABASE_ID]):
    raise ValueError("ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚è¯·ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç¯å¢ƒå˜é‡éƒ½å·²æ­£ç¡®é…ç½®ã€‚")

# åˆå§‹åŒ–å®¢æˆ·ç«¯
cloudflare_client = Cloudflare(api_token=CLOUDFLARE_API_TOKEN)

github_headers = {
    "Authorization": f"token {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

# ================================
# ğŸ” åŸºäºæŒ‡æ ‡çš„æœç´¢æ‰§è¡Œ
# ================================

def execute_metrics_based_search(strategy, keywords, time_ranges):
    """åŸºäºæŒ‡æ ‡æ‰§è¡Œæœç´¢"""
    
    # æ„å»ºæŸ¥è¯¢
    if strategy["name"] == "star_projects":
        query = strategy["query_template"].format(
            keywords=keywords,
            recent_date=time_ranges["recent"]
        )
    elif strategy["name"] == "emerging_projects":
        query = strategy["query_template"].format(
            keywords=keywords, 
            recent_date=time_ranges["recent"]
        )
    elif strategy["name"] == "active_projects":
        query = strategy["query_template"].format(
            keywords=keywords,
            very_recent_date=time_ranges["very_recent"]
        )
    elif strategy["name"] == "potential_projects":
        query = strategy["query_template"].format(
            keywords=keywords,
            recent_date=time_ranges["recent"],
            very_recent_date=time_ranges["very_recent"]
        )
    else:
        query = strategy["query_template"].format(keywords=keywords)
    
    # æ·»åŠ åŸºç¡€è¿‡æ»¤æ¡ä»¶
    query += " is:public archived:false"
    
    params = {
        "q": query,
        "sort": "stars",
        "order": "desc",
        "per_page": 100
    }
    
    try:
        print(f"ğŸ” {strategy['name']}: {keywords[:30]}")
        print(f"   æŸ¥è¯¢: {query[:80]}...")
        
        response = requests.get(
            "https://api.github.com/search/repositories",
            headers=github_headers,
            params=params
        )
        response.raise_for_status()
        
        data = response.json()
        repos = data.get("items", [])
        total_count = data.get("total_count", 0)
        
        print(f"   ğŸ“Š æ‰¾åˆ° {total_count} ä¸ªé¡¹ç›®ï¼Œè·å–å‰ {len(repos)} ä¸ª")
        
        return repos
        
    except requests.exceptions.RequestException as e:
        print(f"   âŒ æœç´¢å¤±è´¥: {e}")
        return []

def enhance_repo_with_metrics(repo):
    """ç”¨æŒ‡æ ‡æ•°æ®å¢å¼ºä»“åº“ä¿¡æ¯"""
    
    # è®¡ç®—ç»¼åˆè¯„åˆ†
    comprehensive_score = calculate_comprehensive_score(repo)
    
    # AIç‰¹å®šè¯„åˆ†
    ai_score = calculate_ai_specific_score(repo)
    
    # æœ€ç»ˆè¯„åˆ†
    final_score = min(50, comprehensive_score + ai_score)
    
    # é¡¹ç›®åˆ†ç±»
    category = classify_by_metrics(repo)
    
    # è´¨é‡ç­‰çº§
    quality_level = determine_quality_level(final_score)
    
    # æ´»è·ƒåº¦çŠ¶æ€
    activity_status = determine_activity_status(repo)
    
    # ç¤¾åŒºå½±å“åŠ›
    community_impact = calculate_community_impact(repo)
    
    enhanced_repo = repo.copy()
    enhanced_repo.update({
        "comprehensive_score": comprehensive_score,
        "ai_specific_score": ai_score,
        "final_score": final_score,
        "category": category,
        "quality_level": quality_level,
        "activity_status": activity_status,
        "community_impact": community_impact,
        "metrics_timestamp": datetime.now().isoformat()
    })
    
    return enhanced_repo

def calculate_ai_specific_score(repo):
    """è®¡ç®—AIé¢†åŸŸç‰¹å®šè¯„åˆ†"""
    name = repo.get("name", "").lower()
    description = repo.get("description", "").lower() if repo.get("description") else ""
    full_text = f"{name} {description}"
    
    ai_score = 0
    
    # AIæŒ‡æ ‡è¯„åˆ†
    for indicator, config in AI_SPECIFIC_METRICS["ai_indicators"].items():
        matches = sum(1 for keyword in config["keywords"] if keyword in full_text)
        if matches > 0:
            ai_score += config["weight"]
    
    # æŠ€æœ¯æ ˆåŠ åˆ†
    for tech, bonus in AI_SPECIFIC_METRICS["tech_stack_bonus"].items():
        if tech in full_text:
            ai_score += bonus
    
    return min(15, ai_score)  # AIç‰¹å®šè¯„åˆ†æœ€é«˜15åˆ†

def classify_by_metrics(repo):
    """åŸºäºæŒ‡æ ‡æ•°æ®è¿›è¡Œé¡¹ç›®åˆ†ç±»"""
    name = repo.get("name", "").lower()
    description = repo.get("description", "").lower() if repo.get("description") else ""
    stars = repo.get("stargazers_count", 0)
    forks = repo.get("forks_count", 0)
    
    full_text = f"{name} {description}"
    
    # åŸºäºæ˜Ÿæ ‡å’Œåˆ†å‰æ•°åˆ¤æ–­é¡¹ç›®ç±»å‹
    if stars >= 1000:
        project_tier = "æ˜æ˜Ÿé¡¹ç›®"
    elif stars >= 500:
        project_tier = "ä¼˜ç§€é¡¹ç›®"
    elif stars >= 100:
        project_tier = "è‰¯å¥½é¡¹ç›®"
    else:
        project_tier = "æ–°å…´é¡¹ç›®"
    
    # åŸºäºå†…å®¹åˆ¤æ–­æŠ€æœ¯åˆ†ç±»
    if any(keyword in full_text for keyword in ["llm", "gpt", "language model", "transformer"]):
        if any(keyword in full_text for keyword in ["api", "server", "serving"]):
            tech_category = "LLMæœåŠ¡"
        elif any(keyword in full_text for keyword in ["chat", "assistant", "bot"]):
            tech_category = "LLMåº”ç”¨"
        else:
            tech_category = "LLMç ”ç©¶"
    elif any(keyword in full_text for keyword in ["diffusion", "stable-diffusion", "image generation"]):
        tech_category = "ç”Ÿæˆå¼AI"
    elif any(keyword in full_text for keyword in ["computer vision", "object detection", "yolo"]):
        tech_category = "è®¡ç®—æœºè§†è§‰"
    elif any(keyword in full_text for keyword in ["rag", "retrieval", "vector", "embedding"]):
        tech_category = "RAGæŠ€æœ¯"
    elif any(keyword in full_text for keyword in ["data science", "analytics", "visualization"]):
        tech_category = "æ•°æ®ç§‘å­¦"
    else:
        tech_category = "é€šç”¨AI"
    
    return f"{tech_category} - {project_tier}"

def determine_quality_level(score):
    """æ ¹æ®è¯„åˆ†ç¡®å®šè´¨é‡ç­‰çº§"""
    if score >= 40:
        return "é¡¶çº§é¡¹ç›® (40+ åˆ†)"
    elif score >= 30:
        return "ä¼˜ç§€é¡¹ç›® (30-39 åˆ†)"
    elif score >= 20:
        return "è‰¯å¥½é¡¹ç›® (20-29 åˆ†)"
    elif score >= 10:
        return "æ½œåŠ›é¡¹ç›® (10-19 åˆ†)"
    else:
        return "åŸºç¡€é¡¹ç›® (< 10 åˆ†)"

def determine_activity_status(repo):
    """ç¡®å®šé¡¹ç›®æ´»è·ƒçŠ¶æ€"""
    pushed_at = repo.get("pushed_at", "")
    
    if not pushed_at:
        return "æœªçŸ¥çŠ¶æ€"
    
    try:
        pushed_date = datetime.fromisoformat(pushed_at.replace('Z', '+00:00'))
        days_ago = (datetime.now(pushed_date.tzinfo) - pushed_date).days
        
        if days_ago <= 7:
            return "ææ´»è·ƒ (7å¤©å†…æ›´æ–°)"
        elif days_ago <= 30:
            return "æ´»è·ƒ (30å¤©å†…æ›´æ–°)"
        elif days_ago <= 90:
            return "ä¸­ç­‰æ´»è·ƒ (90å¤©å†…æ›´æ–°)"
        elif days_ago <= 365:
            return "ä¸å¤Ÿæ´»è·ƒ (ä¸€å¹´å†…æ›´æ–°)"
        else:
            return "ä¸æ´»è·ƒ (è¶…è¿‡ä¸€å¹´)"
    except:
        return "çŠ¶æ€è§£æå¤±è´¥"

def calculate_community_impact(repo):
    """è®¡ç®—ç¤¾åŒºå½±å“åŠ›"""
    stars = repo.get("stargazers_count", 0)
    forks = repo.get("forks_count", 0)
    
    if stars == 0:
        return "æ— å½±å“åŠ›"
    
    fork_ratio = forks / stars if stars > 0 else 0
    
    if stars >= 1000 and fork_ratio >= 0.1:
        return "é«˜å½±å“åŠ›"
    elif stars >= 500 and fork_ratio >= 0.05:
        return "ä¸­ç­‰å½±å“åŠ›"  
    elif stars >= 100:
        return "ä¸€å®šå½±å“åŠ›"
    else:
        return "åˆæ­¥å½±å“åŠ›"

# ================================
# ğŸ¯ æ•°æ®å¤„ç†å’Œä¿å­˜
# ================================

def filter_and_process_repos(repos):
    """è¿‡æ»¤å’Œå¤„ç†ä»“åº“æ•°æ®"""
    processed = []
    
    for repo in repos:
        try:
            # åŸºç¡€è¿‡æ»¤
            if repo.get("fork", False):
                continue
            
            if repo.get("archived", False):
                continue
            
            # æè¿°é•¿åº¦è¿‡æ»¤
            description = repo.get("description", "") or ""
            if len(description) < 15:
                continue
            
            # ä½¿ç”¨æŒ‡æ ‡å¢å¼º
            enhanced_repo = enhance_repo_with_metrics(repo)
            
            # è´¨é‡è¿‡æ»¤ï¼šåªä¿ç•™10åˆ†ä»¥ä¸Šçš„é¡¹ç›®
            if enhanced_repo["final_score"] < 10:
                continue
            
            # æ„å»ºä¿å­˜æ•°æ®
            processed_repo = {
                "id": str(repo.get("id")),
                "name": repo.get("name"),
                "owner": repo.get("owner", {}).get("login"),
                "stars": repo.get("stargazers_count", 0),
                "forks": repo.get("forks_count", 0),
                "description": description,
                "url": repo.get("html_url"),
                "created_at": repo.get("created_at"),
                "updated_at": repo.get("pushed_at"),
                "category": enhanced_repo["category"],
                "tags": extract_metrics_tags(enhanced_repo),
                "summary": generate_metrics_summary(enhanced_repo),
                "relevance_score": enhanced_repo["final_score"]
            }
            
            processed.append(processed_repo)
            
        except Exception as e:
            print(f"âŒ å¤„ç†é¡¹ç›®å‡ºé”™: {e}")
            continue
    
    return processed

def extract_metrics_tags(enhanced_repo):
    """åŸºäºæŒ‡æ ‡æå–æ ‡ç­¾"""
    tags = []
    
    # è´¨é‡æ ‡ç­¾
    if enhanced_repo["final_score"] >= 40:
        tags.append("é¡¶çº§é¡¹ç›®")
    elif enhanced_repo["final_score"] >= 30:
        tags.append("ä¼˜ç§€é¡¹ç›®")
    
    # æ´»è·ƒåº¦æ ‡ç­¾
    if "ææ´»è·ƒ" in enhanced_repo["activity_status"]:
        tags.append("é«˜æ´»è·ƒ")
    elif "æ´»è·ƒ" in enhanced_repo["activity_status"]:
        tags.append("æ´»è·ƒ")
    
    # å½±å“åŠ›æ ‡ç­¾
    if enhanced_repo["community_impact"] == "é«˜å½±å“åŠ›":
        tags.append("é«˜å½±å“åŠ›")
    
    # æŠ€æœ¯æ ‡ç­¾
    category = enhanced_repo["category"]
    if "LLM" in category:
        tags.append("LLM")
    if "ç”Ÿæˆå¼AI" in category:
        tags.append("ç”Ÿæˆå¼AI")
    if "è®¡ç®—æœºè§†è§‰" in category:
        tags.append("è®¡ç®—æœºè§†è§‰")
    
    return ", ".join(tags[:5])

def generate_metrics_summary(enhanced_repo):
    """ç”ŸæˆåŸºäºæŒ‡æ ‡çš„æ‘˜è¦"""
    repo = enhanced_repo
    name = repo.get("name", "")
    score = repo["final_score"]
    quality = repo["quality_level"].split()[0]
    activity = repo["activity_status"].split()[0]
    
    summary = f"{name} - {quality}Â·{activity}Â·{score}åˆ†"
    
    # æ·»åŠ æè¿°ç‰‡æ®µ
    description = repo.get("description", "")
    if description:
        desc_snippet = description.split('.')[0][:50]
        summary += f" | {desc_snippet}"
    
    return summary[:150]

def save_to_database_with_metrics(repos_data):
    """ä¿å­˜æŒ‡æ ‡å¢å¼ºçš„æ•°æ®åˆ°æ•°æ®åº“"""
    if not repos_data:
        print("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
        return
    
    print(f"ğŸ’¾ å¼€å§‹ä¿å­˜ {len(repos_data)} æ¡åŸºäºæŒ‡æ ‡çš„æ•°æ®...")
    
    sql = """
    INSERT INTO repos (id, name, owner, stars, forks, description, url, created_at, updated_at, category, tags, summary, relevance_score)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ON CONFLICT(id) DO UPDATE SET
        stars=excluded.stars,
        updated_at=excluded.updated_at,
        category=excluded.category,
        tags=excluded.tags,
        summary=excluded.summary,
        relevance_score=excluded.relevance_score,
        sync_time=CURRENT_TIMESTAMP;
    """
    
    success_count = 0
    
    for repo in repos_data:
        try:
            params = [
                repo["id"], repo["name"], repo["owner"],
                repo["stars"], repo["forks"], repo["description"],
                repo["url"], repo["created_at"], repo["updated_at"],
                repo["category"], repo["tags"], repo["summary"],
                repo["relevance_score"]
            ]
            
            response = cloudflare_client.d1.database.query(
                database_id=D1_DATABASE_ID,
                account_id=CLOUDFLARE_ACCOUNT_ID,
                sql=sql,
                params=params
            )
            
            success_count += 1
            
            if success_count % 20 == 0:
                print(f"ğŸ“Š å·²ä¿å­˜ {success_count}/{len(repos_data)} æ¡æ•°æ®")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            continue
    
    print(f"ğŸ‰ æˆåŠŸä¿å­˜ {success_count} æ¡åŸºäºæŒ‡æ ‡çš„è®°å½•ï¼")
    return success_count

# ================================
# ğŸš€ ä¸»ç¨‹åº
# ================================

def main_metrics_based_collection():
    """åŸºäºæŒ‡æ ‡çš„ä¸»æ•°æ®æ”¶é›†ç¨‹åº"""
    start_time = datetime.now()
    print("ğŸš€ åŸºäºGitHubæŒ‡æ ‡çš„AIé¡¹ç›®æ”¶é›†å¼€å§‹")
    print(f"â° å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # è·å–æœç´¢ç­–ç•¥å’Œå…³é”®è¯
    search_strategies, time_ranges = build_enhanced_search_queries()
    
    # AIå…³é”®è¯
    ai_keywords = [
        "LLM", "transformer", "GPT", "artificial-intelligence",
        "machine-learning", "deep-learning", "computer-vision",
        "diffusion", "RAG", "pytorch", "tensorflow", "huggingface"
    ]
    
    print(f"ğŸ“Š æœç´¢ç­–ç•¥: {len(search_strategies)} ç§")
    print(f"ğŸ”‘ å…³é”®è¯: {len(ai_keywords)} ä¸ª")
    print(f"ğŸ¯ ç›®æ ‡: {SEARCH_OPTIMIZATION_CONFIG['collection_targets']['high_quality']} æ¡é«˜è´¨é‡æ•°æ®")
    
    all_repos = []
    unique_repos = {}
    search_count = 0
    
    # æŒ‰é…é¢åˆ†é…æœç´¢
    allocation = SEARCH_OPTIMIZATION_CONFIG["daily_search_allocation"]
    
    for strategy in search_strategies:
        strategy_quota = allocation.get(strategy["name"], 10)
        strategy_keywords = ai_keywords[:strategy_quota]
        
        print(f"\nğŸ¯ æ‰§è¡Œç­–ç•¥: {strategy['name']} (é…é¢: {strategy_quota})")
        
        for keyword in strategy_keywords:
            repos = execute_metrics_based_search(strategy, keyword, time_ranges)
            
            # å»é‡å¤„ç†
            for repo in repos:
                repo_id = str(repo.get("id"))
                if repo_id not in unique_repos:
                    unique_repos[repo_id] = repo
            
            search_count += 1
            
            # è¿›åº¦æ˜¾ç¤º
            if search_count % 5 == 0:
                print(f"ğŸ“ˆ å·²æœç´¢ {search_count} æ¬¡ï¼Œæ”¶é›† {len(unique_repos)} ä¸ªå”¯ä¸€é¡¹ç›®")
            
            # APIé™åˆ¶æ§åˆ¶
            time.sleep(SEARCH_OPTIMIZATION_CONFIG["api_limits"]["delay_between_calls"])
            
            # è¾¾åˆ°ç›®æ ‡åå¯æ—©åœ
            if len(unique_repos) >= 800:
                print(f"âœ… å·²æ”¶é›†è¶³å¤Ÿå€™é€‰æ•°æ® ({len(unique_repos)}ä¸ª)")
                break
        
        if len(unique_repos) >= 800:
            break
    
    all_repos = list(unique_repos.values())
    
    print(f"\nğŸ‰ æœç´¢é˜¶æ®µå®Œæˆï¼")
    print(f"ğŸ“Š æ€»æœç´¢æ¬¡æ•°: {search_count}")
    print(f"ğŸ“¦ å€™é€‰é¡¹ç›®: {len(all_repos)} ä¸ª")
    
    # å¤„ç†å’Œè¿‡æ»¤
    print(f"\nğŸ” å¼€å§‹åŸºäºæŒ‡æ ‡å¤„ç†æ•°æ®...")
    filtered_repos = filter_and_process_repos(all_repos)
    
    print(f"âœ… æŒ‡æ ‡è¿‡æ»¤å: {len(filtered_repos)} ä¸ªé¡¹ç›®")
    
    if filtered_repos:
        # æŒ‰è¯„åˆ†æ’åº
        filtered_repos.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        # ä¿å­˜æ•°æ®
        saved_count = save_to_database_with_metrics(filtered_repos)
        
        # ç»Ÿè®¡åˆ†æ
        end_time = datetime.now()
        duration = end_time - start_time
        
        print("\n" + "=" * 60)
        print("ğŸ‰ åŸºäºæŒ‡æ ‡çš„æ•°æ®æ”¶é›†å®Œæˆï¼")
        print(f"â° æ€»è€—æ—¶: {duration}")
        print(f"ğŸ“Š å€™é€‰é¡¹ç›®: {len(all_repos)}")
        print(f"âœ… æœ‰æ•ˆé¡¹ç›®: {len(filtered_repos)}")
        print(f"ğŸ’¾ ä¿å­˜æˆåŠŸ: {saved_count}")
        print(f"ğŸ“ˆ æœ‰æ•ˆç‡: {(len(filtered_repos)/len(all_repos)*100):.1f}%")
        
        # è´¨é‡åˆ†æ
        score_ranges = {"40+åˆ†": 0, "30-39åˆ†": 0, "20-29åˆ†": 0, "10-19åˆ†": 0}
        category_stats = {}
        
        for repo in filtered_repos:
            score = repo["relevance_score"]
            if score >= 40:
                score_ranges["40+åˆ†"] += 1
            elif score >= 30:
                score_ranges["30-39åˆ†"] += 1
            elif score >= 20:
                score_ranges["20-29åˆ†"] += 1
            else:
                score_ranges["10-19åˆ†"] += 1
            
            category = repo["category"].split(" - ")[0]
            category_stats[category] = category_stats.get(category, 0) + 1
        
        print(f"\nğŸ¯ è´¨é‡åˆ†å¸ƒ:")
        for range_name, count in score_ranges.items():
            print(f"   {range_name}: {count} ä¸ª")
        
        print(f"\nğŸ“‹ æŠ€æœ¯åˆ†å¸ƒ:")
        for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"   {category}: {count} ä¸ª")
        
        # æˆåŠŸåˆ¤æ–­
        high_quality_count = score_ranges["40+åˆ†"] + score_ranges["30-39åˆ†"]
        if len(filtered_repos) >= 200:
            print(f"\nğŸŠ æˆåŠŸè¾¾æˆç›®æ ‡ï¼æ”¶é›†åˆ° {len(filtered_repos)} æ¡æ•°æ®ï¼Œå…¶ä¸­ {high_quality_count} æ¡ä¸ºé«˜è´¨é‡é¡¹ç›®ï¼")
        else:
            print(f"\nğŸ“ˆ æ”¶é›†åˆ° {len(filtered_repos)} æ¡æ•°æ®ï¼Œå…¶ä¸­ {high_quality_count} æ¡ä¸ºé«˜è´¨é‡é¡¹ç›®")
    
    else:
        print("âŒ æ²¡æœ‰ç¬¦åˆæŒ‡æ ‡è¦æ±‚çš„é¡¹ç›®")

if __name__ == "__main__":
    main_metrics_based_collection()
