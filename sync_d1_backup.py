import os
import requests
import json
from datetime import datetime, timedelta
from cloudflare import Cloudflare
from dotenv import load_dotenv
from search_config import (
    SEARCH_CONFIG, LLM_TERMS, RAG_TERMS, DIFFUSION_TERMS, 
    ML_TERMS, CV_TERMS, DS_TERMS, ENABLE_DOMAINS, 
    PREFERRED_LANGUAGES, AI_RELEVANCE_THRESHOLD
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- é…ç½®éƒ¨åˆ† ---
# è¯·ç¡®ä¿ä½ çš„ç¯å¢ƒå˜é‡å·²æ­£ç¡®è®¾ç½®
# GITHUB_TOKEN: GitHub ä¸ªäººè®¿é—®ä»¤ç‰Œ
# CLOUDFLARE_API_TOKEN: Cloudflare API ä»¤ç‰Œ
# CLOUDFLARE_ACCOUNT_ID: Cloudflare è´¦æˆ· ID
# D1_DATABASE_ID: Cloudflare D1 æ•°æ®åº“ ID
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
CLOUDFLARE_API_TOKEN = os.environ.get("CLOUDFLARE_API_TOKEN")
CLOUDFLARE_ACCOUNT_ID = os.environ.get("CLOUDFLARE_ACCOUNT_ID")
D1_DATABASE_ID = os.environ.get("D1_DATABASE_ID")

if not all([GITHUB_TOKEN, CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID, D1_DATABASE_ID]):
    raise ValueError("ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚è¯·ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç¯å¢ƒå˜é‡éƒ½å·²æ­£ç¡®é…ç½®ã€‚")

# --- åˆå§‹åŒ– API å®¢æˆ·ç«¯ ---
cloudflare_client = Cloudflare(api_token=CLOUDFLARE_API_TOKEN)

# --- GitHub API è®¾ç½® ---
url = "https://api.github.com/search/repositories"
headers = {
    "Authorization": f"token {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

# åŠ¨æ€ç”ŸæˆæŸ¥è¯¢æ—¥æœŸèŒƒå›´ï¼ˆæœ€è¿‘7å¤©ï¼‰
end_date = datetime.now()
start_date = end_date - timedelta(days=7)
date_range = f"{start_date.strftime('%Y-%m-%d')}..{end_date.strftime('%Y-%m-%d')}"

# æ„å»ºæœç´¢æŸ¥è¯¢
def build_search_query():
    """æ„å»ºå‰æ²¿AIæŠ€æœ¯çš„æœç´¢æŸ¥è¯¢"""
    all_terms = []
    
    # æ ¹æ®é…ç½®æ·»åŠ AIé¢†åŸŸçš„å…³é”®è¯ï¼ˆç²¾ç®€ç‰ˆï¼Œé¿å…æŸ¥è¯¢è¿‡é•¿ï¼‰
    if ENABLE_DOMAINS.get("LLM", True):
        all_terms.append("LLM")
    if ENABLE_DOMAINS.get("RAG", True):
        all_terms.append("RAG")
    if ENABLE_DOMAINS.get("Diffusion", True):
        all_terms.append("diffusion-model")
    if ENABLE_DOMAINS.get("MachineLearning", True):
        all_terms.append("machine-learning")
    if ENABLE_DOMAINS.get("ComputerVision", True):
        all_terms.append("computer-vision")
    if ENABLE_DOMAINS.get("DataScience", True):
        all_terms.append("data-science")
    
    # åŠ¨æ€ç”ŸæˆæŸ¥è¯¢æ—¥æœŸèŒƒå›´
    end_date = datetime.now()
    start_date = end_date - timedelta(days=SEARCH_CONFIG["days_back"])
    date_range = f"{start_date.strftime('%Y-%m-%d')}..{end_date.strftime('%Y-%m-%d')}"
    
    # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
    keywords = " OR ".join(all_terms)
    query = f"({keywords}) created:{date_range} stars:>{SEARCH_CONFIG['min_stars']}"
    
    return query

# æœç´¢å‚æ•°
params = {
    "q": build_search_query(),
    "sort": "stars", 
    "order": "desc",
    "per_page": SEARCH_CONFIG["per_page"]
}

# --- å‡½æ•°ï¼šæ™ºèƒ½åˆ†ç±»AIé¡¹ç›® ---
def categorize_ai_project(name, description):
    """æ ¹æ®é¡¹ç›®åç§°å’Œæè¿°æ™ºèƒ½åˆ†ç±»AIé¡¹ç›®"""
    name_lower = name.lower()
    desc_lower = description.lower() if description else ""
    full_text = f"{name_lower} {desc_lower}"
    
    # AIé¡¹ç›®åˆ†ç±»è§„åˆ™
    if any(keyword in full_text for keyword in ["llm", "large language model", "gpt", "chat", "conversation", "inference"]):
        if any(keyword in full_text for keyword in ["server", "api", "cli", "tool", "framework", "engine"]):
            return "LLMæœåŠ¡ä¸å·¥å…·"
        elif any(keyword in full_text for keyword in ["chat", "app", "client", "ui", "interface"]):
            return "LLMåº”ç”¨"
        else:
            return "LLMç ”ç©¶"
    
    elif any(keyword in full_text for keyword in ["rag", "retrieval", "vector", "embedding", "semantic search", "knowledge"]):
        return "RAGæŠ€æœ¯"
    
    elif any(keyword in full_text for keyword in ["diffusion", "stable-diffusion", "text-to-image", "generation", "dall-e"]):
        return "ç”Ÿæˆå¼AI"
    
    elif any(keyword in full_text for keyword in ["computer vision", "object detection", "image recognition", "opencv", "yolo", "image classification", "segmentation", "face recognition", "ocr"]):
        return "è®¡ç®—æœºè§†è§‰"
    
    elif any(keyword in full_text for keyword in ["data science", "data analysis", "data visualization", "pandas", "numpy", "matplotlib", "jupyter", "statistical", "analytics"]):
        return "æ•°æ®ç§‘å­¦"
    
    elif any(keyword in full_text for keyword in ["machine learning", "scikit-learn", "tensorflow", "pytorch", "keras", "gradient boosting", "random forest", "svm", "neural network"]):
        if any(keyword in full_text for keyword in ["llm", "language model", "gpt", "transformer"]):
            return "LLMç ”ç©¶"  # å¦‚æœåŒæ—¶åŒ…å«MLå’ŒLLMå…³é”®è¯ï¼Œå½’ç±»ä¸ºLLM
        else:
            return "æœºå™¨å­¦ä¹ "
    
    elif any(keyword in full_text for keyword in ["dataset", "benchmark", "toolkit", "framework", "library", "collection"]):
        return "AIèµ„æºä¸å·¥å…·"
    
    elif any(keyword in full_text for keyword in ["mobile", "ios", "android", "flutter", "swift"]):
        return "ç§»åŠ¨ç«¯AI"
    
    elif any(keyword in full_text for keyword in ["reinforcement", "agent", "autonomous"]):
        return "æ™ºèƒ½ä½“ä¸å¼ºåŒ–å­¦ä¹ "
    
    else:
        return "å…¶ä»–AIæŠ€æœ¯"

def generate_summary(name, description):
    """ç”Ÿæˆé¡¹ç›®ç®€è¦ä»‹ç»"""
    if not description:
        return f"{name} - AIç›¸å…³é¡¹ç›®"
    
    # ç®€åŒ–æè¿°ï¼Œæå–å…³é”®ä¿¡æ¯
    desc = description.strip()
    if len(desc) > 100:
        # æˆªå–å‰100ä¸ªå­—ç¬¦å¹¶åœ¨åˆé€‚ä½ç½®æˆªæ–­
        truncated = desc[:100]
        last_space = truncated.rfind(' ')
        if last_space > 50:
            desc = truncated[:last_space] + "..."
        else:
            desc = truncated + "..."
    
    return f"{name} - {desc}"

def extract_tags(name, description, category):
    """æå–é¡¹ç›®æ ‡ç­¾"""
    name_lower = name.lower()
    desc_lower = description.lower() if description else ""
    full_text = f"{name_lower} {desc_lower}"
    
    tags = []
    
    # æŠ€æœ¯æ ‡ç­¾
    tech_keywords = {
        "LLM": ["llm", "large language model"],
        "Transformer": ["transformer"],
        "RAG": ["rag", "retrieval"],
        "Diffusion": ["diffusion"],
        "PyTorch": ["pytorch"],
        "TensorFlow": ["tensorflow"],
        "Scikit-Learn": ["scikit-learn", "sklearn"],
        "Keras": ["keras"],
        "OpenAI": ["openai", "gpt"],
        "Hugging Face": ["huggingface", "hf"],
        "Vector DB": ["vector", "embedding"],
        "Chat": ["chat", "conversation"],
        "API": ["api", "server"],
        "Mobile": ["mobile", "ios", "android"],
        "CLI": ["cli", "command"],
        "Research": ["paper", "research"],
        "Benchmark": ["benchmark", "evaluation"],
        "Computer Vision": ["computer vision", "cv", "opencv"],
        "YOLO": ["yolo"],
        "Object Detection": ["object detection", "detection"],
        "Data Science": ["data science", "ds"],
        "Pandas": ["pandas"],
        "NumPy": ["numpy"],
        "Jupyter": ["jupyter"],
        "Visualization": ["visualization", "matplotlib"],
        "Deep Learning": ["deep learning", "dl"],
        "Neural Network": ["neural network", "nn"],
        "Machine Learning": ["machine learning", "ml"]
    }
    
    for tag, keywords in tech_keywords.items():
        if any(keyword in full_text for keyword in keywords):
            tags.append(tag)
    
    return ", ".join(tags[:5])  # æœ€å¤š5ä¸ªæ ‡ç­¾

# --- å‡½æ•°ï¼šè¿‡æ»¤çœŸæ­£çš„ AI/ML ç›¸å…³ä»“åº“ ---
def filter_ai_repos(repos_data):
    """è¿‡æ»¤å‡ºçœŸæ­£ä¸ AI/ML ç›¸å…³çš„ä»“åº“ - å‡çº§ç‰ˆ"""
    
    # é«˜æƒé‡ AI å…³é”®è¯ (å¿…é¡»åŒ¹é…)
    high_priority_keywords = [
        'artificial intelligence', 'machine learning', 'deep learning', 'neural network',
        'computer vision', 'natural language processing', 'reinforcement learning',
        'llm', 'large language model', 'gpt', 'transformer', 'bert', 'llama',
        'pytorch', 'tensorflow', 'keras', 'scikit-learn', 'huggingface',
        'object detection', 'image recognition', 'text classification', 
        'sentiment analysis', 'fine-tuning', 'rag', 'yolo', 'opencv'
    ]
    
    # ä¸­ç­‰æƒé‡å…³é”®è¯
    medium_priority_keywords = [
        'ai', 'ml', 'dl', 'nlp', 'cv', 'data science', 'algorithm',
        'classification', 'regression', 'clustering', 'segmentation',
        'chatbot', 'recommendation', 'anomaly detection', 'feature extraction'
    ]
    
    # æ’é™¤å…³é”®è¯ (è¿™äº›é€šå¸¸ä¸æ˜¯AIé¡¹ç›®)
    exclude_keywords = [
        'ui framework', 'web framework', 'frontend', 'backend', 'api',
        'compose', 'react', 'vue', 'angular', 'css', 'html', 'javascript',
        'website', 'web design', 'mobile app', 'game engine', 'database',
        'blockchain', 'cryptocurrency', 'devops', 'docker', 'kubernetes'
    ]
    
    # ç¼–ç¨‹è¯­è¨€ç›¸å…³ (å•ç‹¬å‡ºç°æ—¶ä¸ç®—AIé¡¹ç›®)
    language_only_keywords = [
        'python tutorial', 'javascript guide', 'java examples', 'c++ basics',
        'programming exercises', 'coding practice', 'algorithm practice'
    ]
    
    filtered_repos = []
    
    for repo in repos_data:
        name = repo.get('name', '').lower()
        description = repo.get('description', '').lower() if repo.get('description') else ''
        full_text = f"{name} {description}"
        
        # è¯„åˆ†ç³»ç»Ÿ
        score = 0
        
        # é«˜æƒé‡å…³é”®è¯åŒ¹é… (+3åˆ†)
        high_matches = sum(1 for keyword in high_priority_keywords if keyword in full_text)
        score += high_matches * 3
        
        # ä¸­ç­‰æƒé‡å…³é”®è¯åŒ¹é… (+1åˆ†)
        medium_matches = sum(1 for keyword in medium_priority_keywords if keyword in full_text)
        score += medium_matches * 1
        
        # æ’é™¤å…³é”®è¯åŒ¹é… (-5åˆ†)
        exclude_matches = sum(1 for keyword in exclude_keywords if keyword in full_text)
        score -= exclude_matches * 5
        
        # çº¯è¯­è¨€æ•™ç¨‹å…³é”®è¯ (-3åˆ†)
        language_matches = sum(1 for keyword in language_only_keywords if keyword in full_text)
        score -= language_matches * 3
        
        # å†³ç­–é€»è¾‘ï¼šä½¿ç”¨é…ç½®ä¸­çš„é˜ˆå€¼
        if score >= AI_RELEVANCE_THRESHOLD:
            # æ·»åŠ æ™ºèƒ½åˆ†ç±»å’Œæ‘˜è¦
            repo['ai_relevance_score'] = score
            repo['category'] = categorize_ai_project(name, description)
            repo['tags'] = extract_tags(name, description, repo['category'])
            repo['summary'] = generate_summary(repo.get('name'), description)
            
            filtered_repos.append(repo)
            print(f"âœ… ä¿ç•™AIä»“åº“ (åˆ†æ•°:{score}) [{repo['category']}]: {repo['summary']}")
        else:
            print(f"âš ï¸ è¿‡æ»¤æ‰ä¸ç›¸å…³ä»“åº“ (åˆ†æ•°:{score}): {repo.get('name')} - {description[:50]}...")
    
    # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
    filtered_repos.sort(key=lambda x: x.get('ai_relevance_score', 0), reverse=True)
    
    return filtered_repos

# --- å‡½æ•°ï¼šå°†æ•°æ®æ’å…¥åˆ° D1 æ•°æ®åº“ ---
def insert_to_d1(repos_data):
    if not repos_data:
        print("æ²¡æœ‰æ•°æ®å¯æ’å…¥ã€‚")
        return

    # å‡†å¤‡ SQL è¯­å¥ï¼ŒåŒ…å«æ–°çš„å­—æ®µ
    sql = """
    INSERT INTO repos (id, name, owner, stars, forks, description, url, created_at, updated_at, category, tags, summary, relevance_score) 
    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?) 
    ON CONFLICT(id) DO UPDATE SET 
      stars=excluded.stars,
      forks=excluded.forks,
      description=excluded.description,
      updated_at=excluded.updated_at,
      category=excluded.category,
      tags=excluded.tags,
      summary=excluded.summary,
      relevance_score=excluded.relevance_score;
    """
    
    # å‡†å¤‡è¦æ‰§è¡Œçš„å‘½ä»¤åˆ—è¡¨
    commands = []
    for repo in repos_data:
        commands.append({
            "sql": sql,
            "params": [
                str(repo.get("id")),
                repo.get("name"),
                repo.get("owner", {}).get("login"),
                repo.get("stargazers_count"),
                repo.get("forks_count"),
                repo.get("description"),
                repo.get("html_url"),
                repo.get("created_at"),
                repo.get("pushed_at"),
                repo.get("category", "å…¶ä»–AIæŠ€æœ¯"),
                repo.get("tags", ""),
                repo.get("summary", ""),
                repo.get("ai_relevance_score", 0)
            ]
        })

    try:
        # é€šè¿‡ Cloudflare API æ‰§è¡Œæ‰¹é‡æ’å…¥
        success_count = 0
        for cmd in commands:
            try:
                response = cloudflare_client.d1.database.query(
                    database_id=D1_DATABASE_ID,
                    account_id=CLOUDFLARE_ACCOUNT_ID,
                    sql=cmd["sql"],
                    params=cmd["params"]
                )
                success_count += 1
            except Exception as single_error:
                print(f"å•æ¡æ•°æ®æ’å…¥å¤±è´¥: {single_error}")
        
        print(f"æˆåŠŸå°† {success_count}/{len(repos_data)} æ¡æ•°æ®æ’å…¥/æ›´æ–°åˆ° D1 æ•°æ®åº“ã€‚")
        
    except Exception as e:
        print(f"ä¸ Cloudflare D1 é€šä¿¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # å°è¯•é€æ¡æ’å…¥ä»¥æé«˜æˆåŠŸç‡
        print("å°è¯•é€æ¡æ’å…¥æ•°æ®...")
        success_count = 0
        for i, repo in enumerate(repos_data):
            try:
                single_response = cloudflare_client.d1.database.query(
                    database_id=D1_DATABASE_ID,
                    account_id=CLOUDFLARE_ACCOUNT_ID,
                    sql=sql,
                    params=[
                        str(repo.get("id")),
                        repo.get("name"),
                        repo.get("owner", {}).get("login"),
                        repo.get("stargazers_count"),
                        repo.get("forks_count"),
                        repo.get("description"),
                        repo.get("html_url"),
                        repo.get("created_at"),
                        repo.get("pushed_at"),
                        repo.get("category", "å…¶ä»–AIæŠ€æœ¯"),
                        repo.get("tags", ""),
                        repo.get("summary", ""),
                        repo.get("ai_relevance_score", 0)
                    ]
                )
                success_count += 1
            except Exception as single_error:
                print(f"ç¬¬ {i+1} æ¡æ•°æ®æ’å…¥å¤±è´¥: {single_error}")
        
        print(f"é€æ¡æ’å…¥å®Œæˆï¼ŒæˆåŠŸ {success_count}/{len(repos_data)} æ¡ã€‚")

# --- ä¸»ç¨‹åº ---
def main():
    try:
        print(f"æ­£åœ¨æŸ¥è¯¢ GitHub ä»“åº“... æŸ¥è¯¢æ¡ä»¶: {params['q']}")
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()

        data = response.json()
        repos = data.get("items", [])

        if not repos:
            print("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–°ä»“åº“ï¼Œä»»åŠ¡ç»“æŸã€‚")
            return
        
        print(f"æ‰¾åˆ° {len(repos)} ä¸ªç¬¦åˆæ¡ä»¶çš„ä»“åº“:")
        for i, repo in enumerate(repos[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ªä»“åº“ä¿¡æ¯
            print(f"{i}. {repo['name']} - â­{repo['stargazers_count']} - {repo['owner']['login']}")
        
        if len(repos) > 5:
            print(f"... è¿˜æœ‰ {len(repos) - 5} ä¸ªä»“åº“")
        
        # è¿‡æ»¤å‡ºçœŸæ­£çš„ AI/ML ç›¸å…³ä»“åº“
        print(f"\nğŸ” æ­£åœ¨è¿‡æ»¤ AI/ML ç›¸å…³ä»“åº“...")
        filtered_repos = filter_ai_repos(repos)
        
        if filtered_repos:
            print(f"\nâœ… è¿‡æ»¤åå‰©ä½™ {len(filtered_repos)} ä¸ªçœŸæ­£çš„ AI/ML ä»“åº“")
            insert_to_d1(filtered_repos)
        else:
            print("\nâš ï¸ è¿‡æ»¤åæ²¡æœ‰æ‰¾åˆ°çœŸæ­£çš„ AI/ML ç›¸å…³ä»“åº“")

    except requests.exceptions.RequestException as e:
        print(f"è¯·æ±‚ GitHub API æ—¶å‡ºé”™: {e}")
    except Exception as e:
        print(f"è„šæœ¬æ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()
